# -*- org-export-babel-evaluate: nil -*-
# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+AUTHOR: Otho José Sirtoli Marcondes, Philippe O. A. Navaux, Lucas Mello Schnorr
#+EMAIL: ojsmarcondes@inf.ufrgs.br, navaux@inf.ufrgs.br, schnorr@inf.ufrgs.br
#+DATE: September 2025
#+STARTUP: overview indent
#+LANGUAGE: pt-br
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+BIBLIOGRAPHY: ./refs.bib

#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [conference, 10pt, final]
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{booktabs}

* Experiments                                                      :noexport:
** Step 1: Block size fixed to 360, matrix size fixed to 14400
Changing
- Two MPI implementations
- Four schedulers
- Four data paritition
Replications
- 10 repetitions (~4 hours)
** Step 2: 1x traces of 8 configurations
Changing
- Two MPI implementations
- Four data partitions
Fixed
- LWS scheduler
- Block size fixed to 360, matrix size fixed to 14400
No replications
** Step 3: Repeat step 1 to have more at least 20 replications
* Analysis                                                         :noexport:
** Block-cyclic
*** Compute the distributions

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
def_node_topology <- function(P = 2, Q = 3)
{
  tibble(p = 0:(P-1)) |>
    crossing(tibble(q = 0:(Q-1))) |>
    mutate(Node = 1:(P*Q))
}
def_matrix_topology <- function(M = 16, N = 16)
{
  tibble(X = 0:(M-1)) |>
    crossing(tibble(Y = 0:(N-1)))
}
def_distribution <- function(df.topo, df.matrix) {
  P = df.topo |> distinct(p) |> nrow()
  Q = df.topo |> distinct(q) |> nrow()
  df.matrix |>
    mutate(p = X %% P, q = Y %% Q) |>
    left_join(df.topo, by = join_by(p, q))
}
tribble(~P, ~Q,
        2, 3,
        3, 2,
        1, 6,
        6, 1) |>
  mutate(KEY = paste0(P, "x", Q)) |>
  mutate(TOPO = map2(P, Q, def_node_topology)) |>
  mutate(M = 16, N = 16) |>
  mutate(MATR = map2(M, N, def_matrix_topology)) |>
  mutate(OUT2 = map2(TOPO, MATR, def_distribution)) -> df
#+end_src

#+RESULTS:
*** Theme

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
default_theme <- function(base_size = 22, expand = 0.0, legend_title = FALSE, skip_x = FALSE) {
  ret <- list()

  ret[[length(ret) + 1]] <- theme_bw(base_size = base_size)
  ret[[length(ret) + 1]] <- theme(
    plot.margin = unit(c(0, 0, 0, 0), "cm"),
    legend.spacing = unit(3, "cm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0, 0, 0, 0)
  )
  ret[[length(ret) + 1]] <- guides(color = guide_legend(nrow = 1))
  if (!legend_title) {
    ret[[length(ret) + 1]] <- theme(legend.title = element_blank())
  }
  return(ret)
}
#+end_src

#+RESULTS:

*** Plot

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
df |>
  select(KEY, OUT2) |>
  unnest(OUT2) |>
  ggplot(aes(x=X,
             xmin=X,
             xmax=X+1,
             y=Y,
             ymin=Y,
             ymax=Y+1,
             fill=as.factor(Node))) +
  scale_fill_brewer(palette = "Set1") +
  geom_rect() +
  default_theme() +
  facet_wrap(~KEY, nrow=1) +
  guides(fill = guide_legend(nrow = 1, override.aes = list(alpha=1))) +
  xlab("X Tile Coordinate") + ylab("Y Tile Coordinate") +
  scale_x_continuous(breaks = seq(0,16, by=3)) +
  scale_y_reverse(breaks = seq(0,16, by=3)) -> p
ggsave("img/bc.pdf", width=13, height=4.5)
#+end_src

#+RESULTS:

** Block size
*** Read

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
bind_rows(
  read_delim("data/block-size.csv", show_col_types=FALSE, progress=FALSE, delim=";"),
  read_delim("data/block-def.csv", show_col_types=FALSE, progress=FALSE, delim=";")
) -> df
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 110 × 16
      Id Function     threads  gpus     P     Q mtxfmt    nb     m     n   lda
   <dbl> <chr>          <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>
 1     0 dgetrf_nopiv      23     0     1     6      0   128 16000 16000 16000
 2     0 dgetrf_nopiv      23     0     1     6      0   256 16000 16000 16000
 3     0 dgetrf_nopiv      23     0     1     6      0   512 16000 16000 16000
 4     0 dgetrf_nopiv      23     0     1     6      0   320 16000 16000 16000
 5     0 dgetrf_nopiv      23     0     1     6      0  1600 16000 16000 16000
 6     0 dgetrf_nopiv      23     0     1     6      0   960 16000 16000 16000
 7     0 dgetrf_nopiv      23     0     1     6      0  1600 16000 16000 16000
 8     0 dgetrf_nopiv      23     0     1     6      0   512 16000 16000 16000
 9     0 dgetrf_nopiv      23     0     1     6      0   320 16000 16000 16000
10     0 dgetrf_nopiv      23     0     1     6      0   256 16000 16000 16000
# ℹ 100 more rows
# ℹ 5 more variables: seedA <dbl>, bump <dbl>, tsub <dbl>, time <dbl>,
#   gflops <dbl>
# ℹ Use `print(n = ...)` to see more rows
#+end_example

*** Theme

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
default_theme <- function(base_size = 22, expand = 0.0, legend_title = FALSE, skip_x = FALSE) {
  ret <- list()

  ret[[length(ret) + 1]] <- theme_bw(base_size = base_size)
  ret[[length(ret) + 1]] <- theme(
    plot.margin = unit(c(0, 0, 0, 0), "cm"),
    legend.spacing = unit(3, "cm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0, 0, 0, 0)
  )
  ret[[length(ret) + 1]] <- guides(color = guide_legend(nrow = 1))
  if (!legend_title) {
    ret[[length(ret) + 1]] <- theme(legend.title = element_blank())
  }
  return(ret)
}
#+end_src

#+RESULTS:

*** Plot

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
df |>
  ggplot(aes(x = factor(nb), y = time)) +
  geom_violin() +
  labs(x = "Block Size [order]",
       y = "Makespan [seconds]") +
  default_theme() +
  ylim(0, NA) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  facet_wrap(~m, strip.position = "right") -> p
ggsave("img/block-size.pdf", width=6, height=5)
#+end_src

#+RESULTS:

** Data distribution and scheduler
*** Read

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
bind_rows(
  read_delim("data/times-SchedPQ.csv", show_col_types=FALSE, progress=FALSE, delim=";")
) |>
  mutate(KEY = paste0(P, "x", Q)) -> df
df
#+end_src

#+RESULTS:
#+begin_example
# A tibble: 160 × 18
      Id Function     threads  gpus     P     Q mtxfmt    nb     m     n   lda
   <dbl> <chr>          <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>
 1     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 2     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 3     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 4     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 5     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 6     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 7     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 8     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
 9     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
10     0 dgetrf_nopiv      23     0     1     6      0    64  8192  8192  8192
# ℹ 150 more rows
# ℹ 7 more variables: seedA <dbl>, bump <dbl>, tsub <dbl>, time <dbl>,
#   gflops <dbl>, scheduler <chr>, KEY <chr>
# ℹ Use `print(n = ...)` to see more rows
#+end_example

*** Theme

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
default_theme <- function(base_size = 22, expand = 0.0, legend_title = FALSE, skip_x = FALSE) {
  ret <- list()

  ret[[length(ret) + 1]] <- theme_bw(base_size = base_size)
  ret[[length(ret) + 1]] <- theme(
    plot.margin = unit(c(0, 0, 0, 0), "cm"),
    legend.spacing = unit(3, "cm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0, 0, 0, 0)
  )
  ret[[length(ret) + 1]] <- guides(color = guide_legend(nrow = 1))
  if (!legend_title) {
    ret[[length(ret) + 1]] <- theme(legend.title = element_blank())
  }
  return(ret)
}
#+end_src

#+RESULTS:

*** Plot (facet by scheduler)

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
df |>
  ggplot(aes(x = factor(KEY), y = time)) +
  geom_violin() +
  labs(x = "Data Partition [PxQ]",
       y = "Makespan [seconds]") +
  default_theme() +
  ylim(0, NA) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
  facet_grid(m~scheduler) -> p1
#+end_src

#+RESULTS:

*** Plot (facet by PxQ)

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
df |>
  ggplot(aes(x = factor(scheduler), y = time)) +
  geom_violin() +
  labs(x = "Scheduler [name]") +
  default_theme() +
  ylim(0, NA) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1),
        axis.title.y = element_blank()) +
  facet_grid(m~KEY) -> p2
#+end_src

#+RESULTS:

*** Make then together

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
library(patchwork)
p1 + plot_spacer() + p2 +
  plot_layout(ncol=3, widths=c(1, 0.01, 1)) -> p
ggsave("img/distrib-scheduler.pdf", width=13, height=5)
#+end_src

#+RESULTS:

** 4\times NMAD LWS traces, changing data distribution
*** Read

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(starvz)
library(arrow)
library(tidyverse)
library(fs)
library(patchwork)
tibble(DIR = dir_ls("nmad")) |>
  separate(DIR, into=c("XX0", "KEY", "JOBID"), remove=FALSE) |>
  mutate(STARVZ = map(DIR, starvz_read)) |>
  mutate(STARVZ = map2(STARVZ, KEY, function(svz, key) {
    svz$Application |> mutate(KEY = key) -> svz$Application
    return(svz)
  })) |>
  mutate(STARVZ = map(STARVZ, function(svz) {
    svz$config$st$aggregation$step <- 100
#    svz$config$st$idleness <- TRUE
    svz$config$st$outliers <- FALSE
    svz$config$st$labels <- "NODES_only"
#    svz$config$st$aggregation$active <- TRUE
    svz$config$st$aggregation$method <- "nodes"
    return(svz) })) |>
  mutate(GC = map(STARVZ, panel_st)) |>
  mutate(GC = map(GC, function(p) { p + coord_cartesian(xlim=c(0, 48000)) } )) |>
  pull(GC) -> p
#wrap_plots(p, ncol=1)
p[[2]]
#p[[1]] + facet_wrap(~KEY)
#+end_src

#+RESULTS:
: Coordinate system already present. Adding new coordinate system, which will
: replace the existing one.
: Coordinate system already present. Adding new coordinate system, which will
: replace the existing one.
: Coordinate system already present. Adding new coordinate system, which will
: replace the existing one.
: Coordinate system already present. Adding new coordinate system, which will
: replace the existing one.

* *Paper*                                                              :ignore:
** Frontpage                                                        :ignore:
#+BEGIN_EXPORT latex 
\title{Impact of Data Distribution and Schedulers for the LU Factorization on Multi-Core Clusters}

\author{
\IEEEauthorblockN{Otho José Sirtoli Marcondes\IEEEauthorrefmark{1},
                  Philippe O. A. Navaux\IEEEauthorrefmark{1},
                  Lucas Mello Schnorr\IEEEauthorrefmark{1}}
\IEEEauthorblockN{\IEEEauthorrefmark{1} Institute of Informatics/PPGC/UFRGS, Porto Alegre, Brazil}
}
#+END_EXPORT

#+LaTeX: \maketitle

** TODO Abstract                                                    :ignore:

#+LaTeX: \begin{abstract}
The performance of dense linear algebra operations on multi-core
clusters depends critically on the interaction between data
distribution strategies and task scheduling policies. This paper
investigates the trade-offs between different block-cyclic (BC) data
partitioning configurations and intra-node scheduling heuristics for
tiled LU factorization on small-scale HPC clusters. We conduct
experiments on a 144-core cluster (six nodes with 2×12 cores each)
using the CHAMELEON dense linear algebra library with StarPU-MPI
runtime system. Our study examines how communication costs imposed by
different MPI middlewares (OpenMPI and MadMPI) interact with various
scheduling heuristics to influence overall execution time. Through
systematic evaluation of data partitioning configurations (1×6, 2×3,
3×2, and 6×1) and comprehensive trace analysis using the StarVZ
visualization framework, we provide insights into optimal tile sizing,
correlations between data partitioning and scheduling strategies, and
methodological considerations for reproducible HPC
experimentation. Our findings demonstrate the complex interplay
between static data distribution choices and dynamic scheduling
decisions, offering practical guidance for optimizing dense linear
algebra computations on modern multi-core cluster architectures.
#+LaTeX: \end{abstract}

** Introduction Plan                                              :noexport:

1. HPC is paramount today, important for so many fields, the more
   frequent platform in this field is clusters equipped with
   multi-core nodes.
2. Dense linear algebra is the basis of so many applications, LU dense
   factorization of large systems of linear equations is part of so
   many applications in so different fields (provide example)
3. To compute the LU factorization in HPC clusters, composed of so
   many nodes, we need to carry out the load balance among the
   machines. Among the many methods that exist, we can use static
   partitioning of the problem, dynamic, and hybrid. Each of them has
   advantages and disatdanvagens. Explain them.
4. The BLock-cyclic static partintionng of the Scalapack package is
   the method that has been used for so much time, it is the de-facto
   method in popularized benchmarks such as the Linpack.
5. Explain how Block-cyclic works, perhaps with an example of 6 nodes
   and a 16x16 tiled matrix. The goal is to use a static partitioning
   that minimizes the communication frontiers thereby reducing
   synchronization costs. Explain the limitation for cases where the
   number of machines are a prime number. This leads to cases where
   not the total number of machines can be used adequately. For
   example, with 6 nodes, there is no ideal solution that truly
   minimizes communications as you can either use 2x3 or 3x2.
6. There exists alternatives for the Block-Cyclic approach such as the
   1D1D, that despite the sophystication and insentitive for the
   number of machines, are much harder to implement for typical pure
   MPI+X cases, because communications need to be take care
   explicitely. Because of this reason, methods such as the 1D1D and
   variants are not widely used.
7. With the ever scalability of large clusters, composed with an
   increasing number of nodes, static partitioning have become an
   permanent issue because of it cannot adapt to runtime variations
   such as heterogeneous node performance, network congestion, or
   varying computational loads that emerge during the factorization
   process.
8. Because of this, hybrid data partitioning have become a modern
   solution. The method combines static partitioning among the nodes
   of the cluster, very frequently done manually by the programmer,
   while dynamic intra-node scheduling automatic heuristics take care
   of tasks attributed to a node. This method has been popularized
   first by runtimes such as StarPU-MPI cite:augonnet2012starpu, and
   then continued in other solutions such as PaRSEC
   cite:bosilca2013parsec, Specx cite:cardosi2025specx, TaskTorrent
   cite:cambier2020tasktorrent, and CHAMELEON
   cite:klinkenberg2020chameleon.
9. This paper focuses on a scenario that combines static data
   partitioning with dynamic task scheduling. By leveraging
   task-based runtimes, we aim to dynamically schedule tasks at
   runtime while maintaining a static block layout of data. This
   approach enables better adaptability to runtime variations, such
   as load imbalance and communication delays, while preserving the
   advantages of a static data distribution.

10. As a case study, we explore the LU factorization, a fundamental
    operation in linear algebra widely used in scientific
    computing. We adopt a block cyclic distribution scheme for the
    input matrix, a method that balances the computational load and
    spreads data evenly across processes. Our goal is to evaluate how
    dynamic scheduling of tasks can improve the performance of LU
    factorization in clusters.
11. Throughout the development of this work, several challenges were
    encountered related to the use of MPI for executing applications
    across multiple nodes. These included: configuration challenges
    with Guix for package management across distributed nodes; issues
    related to the TCP interface in the MPI NewMadeleine
    implementation; and errors when using StarVZ
    cite:pinto2021providing visualization framework with the traces
    collected from the executions (still not resolved).

** Introduction

High-Performance Computing (HPC) is the backbone for groundbreaking
research and innovation across numerous scientific and engineering
disciplines cite:dongarra2017. From computation physics
cite:dongarra2024co, climate modeling and weather prediction to drug
discovery and artificial intelligence, HPC systems enable researchers
to tackle computationally intensive problems that would be impossible
to solve through other ways. The most prevalent architecture in the
HPC domain consists of clusters of nodes, each one equipped with
multi-core processors and accelerators. Together, hundreds or
thousands of interconnected computing nodes work in concert to deliver
unprecedented computational power and memory capacity for large-scale
scientific applications and large language models training.  Dense
linear algebra operations are typical workloads for such large-scale
clusters, with the LU factorization of large systems of linear
equations cite:lu2023 playing a particularly crucial role across
diverse fields.

Achieving optimal load balance among the nodes of a cluster becomes a
critical challenge that directly impacts overall performance of the LU
factorization. The computational workload can be distributed using
three primary approaches, each with distinct advantages and
disadvantages. Static partitioning pre-determines the data
distribution before execution, offering predictable communication
patterns and excellent data locality, but suffers from inability to
adapt to runtime variations and potential load imbalances as the
computation progresses. Dynamic partitioning makes distribution
decisions at runtime based on current system conditions, providing
excellent adaptability and load balancing capabilities, but incurs
significant overhead from frequent communication and scheduling
decisions. Hybrid approaches attempt to combine the benefits of both
methods by using static distribution for data placement while
employing dynamic scheduling for task execution, potentially achieving
both predictable communication patterns and runtime adaptability,
though at the cost of increased implementation complexity.

The block-cyclic (BC) static partitioning scheme implemented in the
ScaLAPACK library cite:blackford1997scalapack has established itself
as the de facto standard method for dense linear algebra computations
in HPC. This approach has gained widespread adoption and recognition
through its use in prestigious benchmarks such as the High Performance
LINPACK (HPL) benchmark cite:dongarra1979linpack, which serves as the
basis for ranking supercomputers in the TOP500 list cite:top500. The
robustness, predictability, and extensive optimization of the
block-cyclic distribution have made it a good solution for scientific
computing applications, despite its inherent limitations in adapting
to modern heterogeneous and dynamic computing environments.

#+name: fig:bc-new
#+caption: Considering a 16\times16 matrix (the X and Y-axis), the four different block-cyclic distributions (facets) with six nodes (colors).
#+attr_latex: :width \linewidth :placement  [!htb]
[[./img/BC.pdf]]

The block-cyclic distribution operates by organizing processors in a
two-dimensional grid and cyclically distributing matrix blocks across
this processor grid to achieve balanced workload distribution. Figure
[[fig:bc-new]] demonstrates a concrete example with six computing nodes to
data partitioning a 16\times16 matrix. There are four different
partitioning strategies (1\times6, 2\times3, 3\times2, and 6\times1) represented by the
facets while the colors represent the six different nodes. The four
strategies represent different communication boundaries trade-offs as
communication operations occur between nearby processors in the grid
according the tiled LU factorization. A significant limitation emerges
when the number of available nodes is a prime number, as it becomes
impossible to create an optimal rectangular processor grid that fully
utilizes all machines while minimizing communication overhead. For
instance, with six nodes, the choice between a 2×3 or 3×2 processor
grid represents a compromise, as neither configuration perfectly
balances computation and communication requirements. For this example,
one would employ only four nodes if solely synchronization costs would
be taken into account.


Alternative approaches to block-cyclic distribution, such as the 1D-1D
cite:beaumont2001static cite:nesi2020communication distribution
method, offer sophisticated solutions that are less sensitive to the
total number of processors and can potentially achieve better
communication patterns for certain problem configurations. Despite
these theoretical advantages and their mathematical elegance in
handling arbitrary processor counts, these methods face significant
practical barriers in typical MPI+X programming environments. The
primary obstacle lies in their implementation complexity, as they
require programmers to explicitly manage intricate communication
patterns, memory layouts, and synchronization protocols that are
automatically handled in block-cyclic schemes. This implementation
burden, combined with the lack of mature software libraries supporting
these alternative distributions, has limited their adoption in
production scientific computing environments where development time
and code reliability are paramount concerns.

With the ever-increasing scale of modern HPC clusters, now composed of
thousands or even hundreds of thousands of computing nodes, static
partitioning approaches have revealed fundamental limitations that
significantly impact performance and scalability. The primary issue
stems from their inability to adapt to the numerous runtime variations
that characterize large-scale distributed computing environments,
including heterogeneous node performance due to manufacturing
variations and thermal throttling, dynamic network congestion caused
by competing applications and system services, and varying
computational loads that emerge during the factorization process as
different parts of the algorithm exhibit different computational
complexities. These dynamic factors can lead to severe load imbalances
that static partitioning cannot address, resulting in idle processors
and suboptimal resource utilization that becomes increasingly
pronounced as cluster sizes grow.

In response to these challenges, hybrid data partitioning strategies
have emerged as a modern solution that attempts to capture the
benefits of both static and dynamic approaches while mitigating their
respective drawbacks. This methodology combines static partitioning
among the nodes of the cluster, typically implemented through manual
programmer decisions, with dynamic intra-node scheduling that employs
automatic runtime heuristics to manage tasks assigned to individual
CPU cores or GPU accelerators. This dual-level approach has been
pioneered and popularized by advanced runtime systems such as
StarPU-MPI cite:augonnet2012starpu, which demonstrated the feasibility
of combining MPI-based inter-node communication with dynamic task
scheduling. Subsequently, this concept has been refined and extended
by other prominent solutions including PaRSEC cite:bosilca2013parsec,
Specx cite:cardosi2025specx, TaskTorrent cite:cambier2020tasktorrent,
and CHAMELEON cite:klinkenberg2020chameleon.

In this paper, we investigate the trade-offs of different data
partitioning strategies, such as the ones of Figure [[fig:bc-new]], in a
small-scale cluster composed of 144 cores -- six nodes of 2\times12 cores
each. We are interested particularly in the combination of 1/ the
communication cost imposed by different MPI middlewares (OpenMPI
cite:gabriel2004open and MadMPI cite:denis2019scalability), and 2/ the
different intra-node task scheduling heuristics, specifically on how
together these factors influence the overall execution time of the
application.  As a realistic case study, we explore the tiled LU
factorization as implemented by the dense linear algebra library
Chameleon cite:chameleon and the traditional block cyclic distribution
scheme for the input matrix. We carry out experiments using
StarPU-MPI, a runtime that possesses many scheduling heuristics for
intra-node resources. The contributions of this article include *(a)* a
preliminary investigation of the best tile size for our target
architecture; *(b)* an overview comparison correlating data partitioning
strategies and scheduling heuristics; *(c)* a detailed analysis with the
StarVZ cite:pinto2021providing visualization framework using gathered
traces from representative executions; and *(d)* a methodological
strategy that includes reproducible tools such as Guix, and
contributions for the MPI NewMadeleine implementation.


# Section [[sec:related]] presents some basic concepts on matrix
# distribution, modern task-based runtimes and related work.
The paper is structured as follows.  Section [[sec:methodology]] details
our methodology and explains how we conducted the experiments in our
investigation. Section [[sec:results]] presents experimental results and
their interpretation, focusing on the performance overview and
comparison of different MPI layers. Section [[sec:conclusion]] concludes
this work with some considerations and future work.

** Methods and Materials
<<sec:methodology>>

Our experimental investigation has three phases to evaluate the
performance characteristics of the tiled LU factorization under
different setups. We describe the methods involved in these three
phrases. We describe the preliminary study to determine the optimal
block size for our target platform, systematically varying tile
dimensions to identify the configuration that minimize the makespan.
Second, we describe the full-factorial experimental design to
investigate the complex interplay between static data distribution
strategies, intra-node scheduling heuristics, and two different MPI
implementations. Third, we detail how we obtain detailed execution
traces collected during representative runs to gain deeper
understanding of the performance differences observed between OpenMPI
and MadMPI implementations.
#+latex: %
We describe 1/ the StarPU-MPI runtime together with the two MPI
implementations (OpenMPI and MadMPI); 2/ the task-based multi-node
implementation of the LU factorization application as available in the
Chameleon suite; 3/ the hardware and software configuration, including
the GUIX mechanism to have a fully reproducible software stack; and 4/
the Design of Experiments of the three phrases described previously.

*** Runtime: task-based StarPU-MPI and the MPI layers

As the computers used in HPC environments became more complex,
adapting and exploiting them to their full potential has become
increasingly challenging. The task-based paradigm was designed to
solve these new challenges. It relies on a DAG (Directed Acyclic
Graph) to represent the relation between tasks and their dependencies
(edges). The runtime scheduler can dynamically allocate these tasks in
execution time, according to the dependencies of the graph and the
scheduler heuristic cite:faverge2023programming.

#+latex: \noindent
*The Runtime*. 
StarPU cite:augonnet2009starpu is a task-based runtime system for
heterogeneous platforms, being multicore or multinode. The StarPU uses
the Sequential Task-Flow (STF) cite:kennedy2001optimizing, where the
tasks are sequentially submitted to the runtime that is responsible
for their scheduling. Each task can have one or more implementations
for each type of computational resources (CPU, GPU), so that the
scheduler assigns to them a task as soon it becomes ready for the
execution when all its dependencies have been satisfied.  The
scheduler heuristics studied in this work are the following:
\verb|lws|: stands for locality work stealing. When a worker becomes
idle, it steals a task from a neighboring worker; \verb|random|: tasks
are distributed randomly according the assumed worker overall
performance; \verb|dmda|: takes task execution performance models and
data transfer time into account; \verb|dmdas|: same as \verb|dmda|,
but also take into account task priorities and data buffer
availability on the target device. The StarPU-MPI extension
cite:augonnet2012starpu enables the the multi-node execution, allowing
the application programmer to mark to which node each task belongs. By
doing so, the runtime infers all the inter-node communication
dependencies for which it employs asynchronous MPI point-to-point
operations using any MPI implementation.

# The Chameleon cite:agullo:inria-00547847 library with. The Chameleon
# code as other linear algebra libraries such as DPLASMA
# cite:bosilca2011flexible are built on task-based runtimes, which
# allows them to efficiently exploit their computational resources of
# clusters.

#+latex: \noindent
*MPI Layers*. In our study, we employ NewMadeleine and OpenMPI as MPI
transport layers for the StarPU-MPI runtime. NewMadeleine
cite:aumage2007new is a high-performance communication library that
provides MPI functionality through its MadMPI interface. Its design
addresses scalability challenges with irregular communication patterns
and high message volumes.  The library is multi-threaded and supports
the MPI_THREAD_MULTIPLE threading level, making it particularly
well-suited for task-based runtime systems like StarPU-MPI that rely
on concurrent communication operations from multiple threads.  OpenMPI
is a widely-adopted, open-source implementation of the Message Passing
Interface (MPI) standard that provides portable, high-performance
parallel communication capabilities.  The comprehensive multi-threaded
compliance makes it a natural baseline for comparative studies.

*** Application: the LU Factorization

We use the Chameleon dense linear algebra cite:chameleon
implementation of the LU factorization cite:lu2023. As shown in the Figure
[[fig:LU-factor]], the LU factorization of a given matrix $A$ is defined
as $A=LU$, where $L$ is a lower triangular matrix and $U$ is an upper
triangular matrix. The LU algorithm relies on three different LAPACK
kernels cite:anderson1999lapack: \verb|DGTRF-NOPIV|, \verb|DTRSM| and
\verb|DGEMM|. This application has a tendency to be dominated by
\verb|DGEMM| kernels when $N$ is large.  The LU parallelization
strategy of Chameleon is very similar to that of ScaLAPACK
cite:blackford1997scalapack, following the task-based paradigm, with
built-in support for block-cyclic data distribution. As typical for
task-based applications, function calls represent task submissions for
a runtime system that handles all scheduling activities. When
programming for StarPU-MPI, the application code must contain the
configuration of the block-cyclic method.  The StarPU heuristics carry
out the task scheduling within a compute node dynamically. Because of
this hybrid mechanism, and the fact that \verb|DGEMM| tasks are the
most common kernel, it becomes mandatory to have well distributed
sub-matrixes between the nodes using a good tile distribution.

#+name: fig:LU-factor
#+caption: The task-based tiled LU algorithm (left) without pivoting, and the regions of A updated at a given iteration k (right) cite:nesi2020communication.
#+attr_latex: :width .5\textwidth
[[./LU-factor.png]]

The block cyclic distribution, also popularized by the ScaLAPACK,
depends on the P \times Q parameters and the number of available
nodes. Based on these configurations, nodes receive tiles of the input
matrix. In the Figure [[fig:bc-new]] we can visualize that while 1\times6,
there is only one node per row, as in reverse of the 6\times1 distribution,
there is only one node per column. For the 2\times4 and 4\times2 cases, the
distribution is cyclic.

*** Hardware & Software configuration

We employ one partition of the PCAD cluster at INF/UFRGS in the
experiments. Table [[tab:hardware]] specifies the hardware of one node.
The \verb|Cei| partition comprises six nodes, where each node uses a
10-Gigabit X540-AT2 network interface connected to a dedicated switch
with sufficient aggregated capacity. From the software perspective, we
use the 1.4.7 StarPU and 1.3.0 Chameleon versions. We also used
NewMadeleine, with the commit =6e1a64d0= (June 2025, after fixes in the
TCP interface reported by us) and the MPI implementation from OpenMPI
5.0.7 cite:gabriel2004open. The complete software stack is kept stable
by the Guix package manager, with two manifest files one for each MPI
layer while sharing the remaining software configurations.

#+CAPTION: Hardware specification of the cei partition.
#+NAME: tab:hardware
#+ATTR_LATEX: :booktabs t
| *Nome* | *CPU*                              | *RAM*        | *Network* |
| <l>  | <l>                              | <l>        | <l>     |
|------+----------------------------------+------------+---------|
| Cei  | 2 x Intel(R) Xeon(R) Silver 4116 | 96 GB DDR4 | 10G     |
|      | 2.10 GHz, 24 physical cores      |            |         |

The data analysis is carried out in a post-mortem fashion by scripts
written in the R language when using the StarVZ library framework
cite:pinto2021providing.

*** Design of Experiments (DoE) and Workload

We have three experimental designs to carry out this investigation on
the influence of static data partitioning and intra-node scheduling
heuristics in the makespan.

# Preliminary study (optimal block size)
# - Using the six nodes, the =lws= scheduler, and the MadMPI layer, an
#   input matrix of order 16K.
# - We vary the block size using tiles of orders 128, 256, 300, 320,
#   460, 400, 420, 512, 960, and 1600.

Our preliminary study focuses on determining the optimal tile size for
LU factorization on our target platform by systematically evaluating
performance across a range of block dimensions. Using all six
available computing nodes with the Locality Work Stealing (LWS)
scheduler and the MadMPI communication layer, we perform tiled LU
factorization on a fixed input matrix of order 16000 (16K). We vary
the tile size across ten different configurations: 128×128, 256×256,
300×300, 320×320, 400×400, 420×420, 460×460, 512×512, 960×960, and
1600×1600. This range encompasses both standard power-of-two
dimensions commonly used in linear algebra libraries and intermediate
sizes that may better exploit the specific memory hierarchy and
computational characteristics of our multi-core cluster
architecture. The selection of tile sizes balances the trade-off
between computational granularity and communication overhead, allowing
us to identify the configuration that maximizes arithmetic intensity
while minimizing synchronization costs for subsequent experiments.

# Full-factorial experimental design
# - Using the six nodes, and an input matrix of order 14.4K
# - We vary three factors:
#   - Two MPI implementations: MadMPI (NMAD), and OpenMPI (OMPI)
#   - Four intra-node schedulers: =lws=, =random=, =dmda=, and =dmdas=
#  - Four inter-node static data partition, considering six nodes: 1\times6, 2\times3, 3\times2, and 6\times1.

Our full-factorial experimental design systematically investigates the
interaction effects between multiple factors influencing LU
factorization performance by evaluating all possible combinations of
three key variables. Using all six computing nodes, a fixed input
matrix of order 14400 (14.4K), and a fixed block size (360\times360), we
manipulate three factors across their complete range of values. The
first factor consists of two MPI implementations: MadMPI (NMAD) and
OpenMPI (OMPI), allowing us to assess the impact of different
communication middleware on overall performance. The second factor
varies four intra-node scheduling heuristics available in StarPU-MPI:
=lws=, =random=, =dmda=, and =dmdas=.  The third factor explores the four
inter-node static data partitioning strategies that organize the six
available nodes into different processor grid configurations: 1×6,
2×3, 3×2, and 6×1, corresponding to the block-cyclic distribution
schemes illustrated in our earlier analysis. This 2×4×4 factorial
design yields 32 unique experimental conditions, enabling
comprehensive analysis of main effects and interaction patterns
between communication layers, scheduling policies, and data
distribution strategies.

# We run each configuration a number of times in a random order
# to quantify variability. As response variables, we observe the
# makespan metric, which represents the total execution time.

For both the preliminary study and the full-factorial experimental
design, we execute each configuration multiple times in randomized
order to quantify performance variability and ensure statistical
reliability of our results. The randomization of execution order helps
mitigate potential systematic biases introduced by temporal factors
such as system load variations, thermal effects, or network congestion
that could influence performance measurements. As our primary response
variable, we measure the makespan metric, which represents the total
execution time from the initiation of the LU factorization algorithm
until its completion, providing a comprehensive measure of overall
computational efficiency that encompasses both computation and
communication costs across all participating nodes.

# Third, we detail how we obtain detailed execution traces collected
# during representative runs to gain deeper understanding of the
# performance differences observed between OpenMPI and MadMPI
# implementations.

# Finally, our third phases encompasses the built-in trace collection
# mechanism of StarPU. We collect eight representative executions (no
# replications) with these configurations.

# Changing
# - Two MPI implementations
# - Four data partitions
# Fixed
# - LWS scheduler
# - Block size fixed to 360, matrix size fixed to 14400
# No replications

Finally, our third phase employs StarPU's built-in trace collection
mechanism to conduct detailed performance analysis through eight
representative executions without replications. We fix the LWS
scheduler, block size at 360×360 tiles, and matrix size at 14,400,
while varying two factors: the MPI implementation (MadMPI and OpenMPI)
and the four data partitioning strategies (1×6, 2×3, 3×2, and 6×1). We
leverage on these cases to enable in-depth examination of delays
induced by longer communication operations and other task scheduling
problems to better elucidate our previously overview of observations.
Our focus here is to illustrate the differences between the two MPI
implementations.

** TODO Results
<<sec:results>>

*** Block Size

This value was taken from a preliminary execution only varying the
block size as shown in Figure~\ref{fig:timeBlocks}, that depicts
different blocks dimensions and their respective execution times. It
is possible to observe that the 360 block size had the best
performance among the other values.

*** To be integrated                                             :noexport:
Figure~\ref{fig:timePQ} depicts four panels, aligned in the X
dimension (time), each showing the execution time of a different
scheduler (\verb|random|, \verb|lws|, \verb|dmdas|, \verb|dmda|) with
a fixed PxQ configuration. The standard deviation is represented by
the black error bars on each bar. We can see that the \verb|lws| and
\verb|random| schedulers did not present much variation when changing
the PxQ configuration. As for the \verb|dmdas| and \verb|dmda|, both
of them showed significantly better performance when utilizing the
$P=2$ $Q=3$ and $P=3$ $Q=2$ configurations.

# NEEDS TO BE REDONE (DONE WITH THE 64 BLOCK SIZE)
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{tempo_medio_com_desvio_padrao_por_PQ.png}
\caption{Execution times based on the PxQ configuration}
\label{fig:timePQ}
\end{figure}

Figure~\ref{fig:timeSched} depicts four panels, aligned in the X dimension (time), each of them showing the execution time of a PxQ configuration with a fixed scheduler heuristic. The standard deviation is represented by the black error bars on each bar. We can see that the \verb|lws| scheduler had the best results among the schedulers fallowed by the \verb|random| scheduler. The \verb|dmda| and \verb|dmdas| had similar performance, with performance gains when P and Q are interleaved.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{tempo_medio_com_desvio_padrao_por_scheduler.png}
\caption{Execution times based on the scheduler heuristic}
\label{fig:timeSched}
\end{figure}

** More                                                           :noexport:
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{719424.pdf}
\caption{Trace of an execution of LU-Factor at cei machine (lws heuristic)}
\label{fig:trace}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{719448.pdf}
\caption{Trace of an execution of LU-Factor at cei machine (dmdas heuristic)}
\label{fig:trace}
\end{figure}

** TODO Conclusion
<<sec:conclusion>>

The study examines the impact of data distribution using Block cyclic
and also the impact of different scheduler heuristics in the context
of task-based runtime in clusters. The linear algebra LU factorization
application provided by Chameleon was used as a means to analyze how
these configurations impact performance. The \verb|dmda| and
\verb|dmdas| heuristics presented similar behavior in their execution
times, showing performance gains when the P and Q were
interleaved. The \verb|lws| heuristic presented the best results in
terms of performance, although the P and Q parameters did not have
significant impact in it. The \verb|random| heuristic also showed no
significant impact on its performance based on the P and Q parameters.

# The various issues encountered during the development of this work caused the executions utilizing NewMadeleine implementation of MPI were only conducted toward the end of the available time. Also, to build a stronger argument about why a given scheduler or distribution outperformed the others, the use of execution traces would be necessary. These traces would make possible to visualize the behavior of the application during its execution. As previously mentioned, there are still ongoing problems in the utilization of the FxT traces with the StarVZ framework. The next steps would consist of: resolve the issues preventing StarVZ usage and use SimGrid cite:CASANOVA2025103125 to run simulations and scale the number of nodes.

#+latex: \noindent
*Acknowledgements*.
#+latex: %
The experiments in this work used the PCAD infrastructure,
http://gppd-hpc.inf.ufrgs.br, at INF/UFRGS.  We also acknowledge the
Brazilian National Council for Scientific Technological Development
(CNPq) for their financial scholarship support. This study was
financed in part by the Coordenação de Aperfeiçoamento de Pessoal de
Nível Superior - Brasil (CAPES) - Finance Code 001, the FAPERGS
(16/354-8, 16/348-8), and Petrobras (2020/00182-5).

** References                                                       :ignore:

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{refs}

* Bibtex                                                           :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib
@inproceedings{denis2019scalability,
  title={Scalability of the NewMadeleine communication library for large numbers of MPI point-to-point requests},
  author={Denis, Alexandre},
  booktitle={2019 19th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID)},
  pages={371--380},
  year={2019},
  organization={IEEE}
}


@article{dongarra2024co,
  title={The co-evolution of computational physics and high-performance computing},
  author={Dongarra, Jack and Keyes, David},
  journal={Nature Reviews Physics},
  volume={6},
  number={10},
  pages={621--627},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@book{blackford1997scalapack,
  title={ScaLAPACK users' guide},
  author={Blackford, L Susan and Choi, Jaeyoung and Cleary, Andy and D'Azevedo, Eduardo and Demmel, James and Dhillon, Inderjit and Dongarra, Jack and Hammarling, Sven and Henry, Greg and Petitet, Antoine and others},
  year={1997},
  publisher={SIAM}
}

@inproceedings{augonnet2009starpu,
  title={StarPU: a unified platform for task scheduling on heterogeneous multicore architectures},
  author={Augonnet, C{\'e}dric and Thibault, Samuel and Namyst, Raymond and Wacrenier, Pierre-Andr{\'e}},
  booktitle={European Conference on Parallel Processing},
  pages={863--874},
  year={2009},
  organization={Springer}
}

@inproceedings{augonnet2012starpu,
  title={StarPU-MPI: Task programming over clusters of machines enhanced with accelerators},
  author={Augonnet, C{\'e}dric and Aumage, Olivier and Furmento, Nathalie and Namyst, Raymond and Thibault, Samuel},
  booktitle={European MPI Users' Group Meeting},
  pages={298--299},
  year={2012},
  organization={Springer}
}

@incollection{agullo:inria-00547847,
  TITLE = {{Faster, Cheaper, Better -- a Hybridization Methodology to Develop Linear Algebra Software for GPUs}},
  AUTHOR = {Agullo, Emmanuel and Augonnet, C{\'e}dric and Dongarra, Jack and Ltaief, Hatem and Namyst, Raymond and Thibault, Samuel and Tomov, Stanimire},
  URL = {https://inria.hal.science/inria-00547847},
  BOOKTITLE = {{GPU Computing Gems}},
  EDITOR = {Wen-mei W. Hwu},
  PUBLISHER = {{Morgan Kaufmann}},
  VOLUME = {2},
  YEAR = {2010},
  MONTH = Sep,
  PDF = {https://inria.hal.science/inria-00547847v1/file/gpucomputinggems_plagma.pdf},
  HAL_ID = {inria-00547847},
  HAL_VERSION = {v1},
}

@book{kennedy2001optimizing,
  title={Optimizing compilers for modern architectures: a dependence-based approach},
  author={Kennedy, Ken and Allen, John R},
  year={2001},
  publisher={Morgan Kaufmann Publishers Inc.}
}

@inproceedings{aumage2007new,
  title={New madeleine: A fast communication scheduling engine for high performance networks},
  author={Aumage, Olivier and Brunet, Elisabeth and Furmento, Nathalie and Namyst, Raymond},
  booktitle={2007 IEEE International Parallel and Distributed Processing Symposium},
  pages={1--8},
  year={2007},
  organization={IEEE}
}
@inproceedings{gabriel2004open,
  title={Open MPI: Goals, concept, and design of a next generation MPI implementation},
  author={Gabriel, Edgar and Fagg, Graham E and Bosilca, George and Angskun, Thara and Dongarra, Jack J and Squyres, Jeffrey M and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and others},
  booktitle={European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting},
  pages={97--104},
  year={2004},
  organization={Springer}
}

@inproceedings{nesi2020communication,
  title={Communication-aware load balancing of the LU factorization over heterogeneous clusters},
  author={Nesi, Lucas Leandro and Schnorr, Lucas Mello and Legrand, Arnaud},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={54--63},
  year={2020},
  organization={IEEE}
}

@article{garcia2018visual,
  title={A visual performance analysis framework for task-based parallel applications running on hybrid clusters},
  author={Garcia Pinto, Vin{\'\i}cius and Mello Schnorr, Lucas and Stanisic, Luka and Legrand, Arnaud and Thibault, Samuel and Danjean, Vincent},
  journal={Concurrency and Computation: Practice and Experience},
  volume={30},
  number={18},
  pages={e4472},
  year={2018},
  publisher={Wiley Online Library}
}

@article{faverge2023programming,
  title={Programming heterogeneous architectures using hierarchical tasks},
  author={Faverge, Mathieu and Furmento, Nathalie and Guermouche, Abdou and Lucas, Gwenol{\'e} and Namyst, Raymond and Thibault, Samuel and Wacrenier, Pierre-andr{\'e}},
  journal={Concurrency and Computation: Practice and Experience},
  volume={35},
  number={25},
  pages={e7811},
  year={2023},
  publisher={Wiley Online Library}
}

@inproceedings{bosilca2011flexible,
  title={Flexible development of dense linear algebra algorithms on massively parallel architectures with DPLASMA},
  author={Bosilca, George and Bouteiller, Aurelien and Danalis, Anthony and Faverge, Mathieu and Haidar, Azzam and Herault, Thomas and Kurzak, Jakub and Langou, Julien and Lemarinier, Pierre and Ltaief, Hatem and others},
  booktitle={2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
  pages={1432--1441},
  year={2011},
  organization={IEEE}
}

@book{anderson1999lapack,
  title={LAPACK users' guide},
  author={Anderson, Edward and Bai, Zhaojun and Bischof, Christian and Blackford, L Susan and Demmel, James and Dongarra, Jack and Du Croz, Jeremy and Greenbaum, Anne and Hammarling, Sven and McKenney, Alan and others},
  year={1999},
  publisher={SIAM}
}

@inproceedings{pinto2021providing,
  title={Providing in-depth performance analysis for heterogeneous task-based applications with starvz},
  author={Pinto, Vin{\'\i}cius Garcia and Nesi, Lucas Leandro and Miletto, Marcelo Cogo and Schnorr, Lucas Mello},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={16--25},
  year={2021},
  organization={IEEE}
}

@article{CASANOVA2025103125,
  title = {{Lowering entry barriers to developing custom simulators of distributed applications and platforms with SimGrid}},
  journal = {Parallel Computing},
  volume = {123},
  pages = {103-125},
  year = {2025},
  issn = {0167-8191},
  doi = {https://doi.org/10.1016/j.parco.2025.103125},
  author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr{\'e}d{\'e}ric},
  keywords = {Simulation of distributed computing systems, SimGrid},
  pdf = {https://hal.science/hal-04909441/file/paper.pdf}
}


@ARTICLE{dongarra2017,
  author={Dongarra, Jack and Tomov, Stanimire and Luszczek, Piotr and Kurzak, Jakub and Gates, Mark and Yamazaki, Ichitaro and Anzt, Hartwig and Haidar, Azzam and Abdelfattah, Ahmad},
  journal={Computing in Science \& Engineering},
  title={With Extreme Computing, the Rules Have Changed},
  year={2017},
  volume={19},
  number={3},
  pages={52-62},
  keywords={Program processors;High performance computing;Symmetric matrices;Parallel processing;Computational modeling;Market research;High-performance computing;exascale;algorithms;scheduling;autotuning;scientific computing},
  doi={10.1109/MCSE.2017.48}
}

@inproceedings{lu2023,
author = {Xia, Yang and Jiang, Peng and Agrawal, Gagan and Ramnath, Rajiv},
title = {End-to-End LU Factorization of Large Matrices on GPUs},
year = {2023},
isbn = {9798400700156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572848.3577486},
doi = {10.1145/3572848.3577486},
abstract = {LU factorization for sparse matrices is an important computing step for many engineering and scientific problems such as circuit simulation. There have been many efforts toward parallelizing and scaling this algorithm, which include the recent efforts targeting the GPUs. However, it is still challenging to deploy a complete sparse LU factorization workflow on a GPU due to high memory requirements and data dependencies. In this paper, we propose the first complete GPU solution for sparse LU factorization. To achieve this goal, we propose an out-of-core implementation of the symbolic execution phase, thus removing the bottleneck due to large intermediate data structures. Next, we propose a dynamic parallelism implementation of Kahn's algorithm for topological sort on the GPUs. Finally, for the numeric factorization phase, we increase the parallelism degree by removing the memory limits for large matrices as compared to the existing implementation approaches. Experimental results show that compared with an implementation modified from GLU 3.0, our out-of-core version achieves speedups of 1.13--32.65X. Further, our out-of-core implementation achieves a speedup of 1.2--2.2 over an optimized unified memory implementation on the GPU. Finally, we show that the optimizations we introduce for numeric factorization turn out to be effective.},
booktitle = {Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
pages = {288–300},
numpages = {13},
keywords = {GPU acceleration, LU factorization, memory limits},
location = {Montreal, QC, Canada},
series = {PPoPP '23}
}

@inproceedings{augonnet2012starpu,
  title={StarPU-MPI: Task programming over clusters of machines enhanced with accelerators},
  author={Augonnet, C{\'e}dric and Aumage, Olivier and Furmento, Nathalie and Namyst, Raymond and Thibault, Samuel},
  booktitle={European MPI Users' Group Meeting},
  pages={298--299},
  year={2012},
  organization={Springer}
}

@ARTICLE{top500,
  author={Strohmaier, Erich and Meuer, Hans W. and Dongarra, Jack and Simon, Horst D.},
  journal={Computer}, 
  title={The TOP500 List and Progress in High-Performance Computing}, 
  year={2015},
  volume={48},
  number={11},
  pages={42-49},
  keywords={High performance computing;scientific computing;high-performance computing;parallel computing;supercomputers;TOP500;Linpack;benchmarks;application performance},
  doi={10.1109/MC.2015.338}}

@book{dongarra1979linpack,
  title={LINPACK users' guide},
  author={Dongarra, Jack J and Moler, Cleve Barry and Bunch, James R and Stewart, Gilbert W},
  year={1979},
  publisher={SIAM}
}

@article{beaumont2001static,
  title={Static LU decomposition on heterogeneous platforms},
  author={Beaumont, Olivier and Legrand, Arnaud and Rastello, Fabrice and Robert, Yves},
  journal={The International Journal of High Performance Computing Applications},
  volume={15},
  number={3},
  pages={310--323},
  year={2001},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}

@inproceedings{nesi2020communication,
  title={Communication-aware load balancing of the LU factorization over heterogeneous clusters},
  author={Nesi, Lucas Leandro and Schnorr, Lucas Mello and Legrand, Arnaud},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={54--63},
  year={2020},
  organization={IEEE}
}


@inproceedings{augonnet2009starpu,
  title={StarPU: a unified platform for task scheduling on heterogeneous multicore architectures},
  author={Augonnet, C{\'e}dric and Thibault, Samuel and Namyst, Raymond and Wacrenier, Pierre-Andr{\'e}},
  booktitle={European Conference on Parallel Processing},
  pages={863--874},
  year={2009},
  organization={Springer}
}

@article{bosilca2013parsec,
  title={Parsec: Exploiting heterogeneity to enhance scalability},
  author={Bosilca, George and Bouteiller, Aurelien and Danalis, Anthony and Faverge, Mathieu and H{\'e}rault, Thomas and Dongarra, Jack J},
  journal={Computing in Science \& Engineering},
  volume={15},
  number={6},
  pages={36--45},
  year={2013},
  publisher={IEEE}
}

@article{cardosi2025specx,
  title={Specx: a C++ task-based runtime system for heterogeneous distributed architectures},
  author={Cardosi, Paul and Bramas, B{\'e}renger},
  journal={PeerJ Computer Science},
  volume={11},
  pages={e2966},
  year={2025},
  publisher={PeerJ Inc.}
}


@inproceedings{cambier2020tasktorrent,
  title={TaskTorrent: a lightweight distributed task-based runtime system in C++},
  author={Cambier, L{\'e}opold and Qian, Yizhou and Darve, Eric},
  booktitle={2020 IEEE/ACM 3rd Annual Parallel Applications Workshop: Alternatives To MPI+ X (PAW-ATM)},
  pages={16--26},
  year={2020},
  organization={IEEE}
}


@article{klinkenberg2020chameleon,
  title={CHAMELEON: reactive load balancing for hybrid MPI+ OpenMP task-parallel applications},
  author={Klinkenberg, Jannis and Samfass, Philipp and Bader, Michael and Terboven, Christian and M{\"u}ller, Matthias S},
  journal={Journal of Parallel and Distributed Computing},
  volume={138},
  pages={55--64},
  year={2020},
  publisher={Elsevier}
}

@incollection{chameleon,
  TITLE = {{Faster, Cheaper, Better -- a Hybridization Methodology to Develop Linear Algebra Software for GPUs}},
  AUTHOR = {Agullo, Emmanuel and Augonnet, C{\'e}dric and Dongarra, Jack and Ltaief, Hatem and Namyst, Raymond and Thibault, Samuel and Tomov, Stanimire},
  URL = {https://inria.hal.science/inria-00547847},
  BOOKTITLE = {{GPU Computing Gems}},
  PUBLISHER = {{Morgan Kaufmann}},
  VOLUME = {2},
  YEAR = {2010},
  MONTH = Sep,
  PDF = {https://inria.hal.science/inria-00547847/file/gpucomputinggems_plagma.pdf},
  HAL_ID = {inria-00547847},
  HAL_VERSION = {v1},
}

@inproceedings{gabriel2004open,
  title={Open MPI: Goals, concept, and design of a next generation MPI implementation},
  author={Gabriel, Edgar and Fagg, Graham E and Bosilca, George and Angskun, Thara and Dongarra, Jack J and Squyres, Jeffrey M and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and others},
  booktitle={European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting},
  pages={97--104},
  year={2004},
  organization={Springer}
}

#+end_src
* Emacs setup                                                      :noexport:

#+BEGIN_SRC elisp
(setq org-export-global-macros
      '((section-name . "(eval (car (org-get-outline-path t)))")
        (subsection-name . "(eval (car (last (org-get-outline-path t))))")))
#+END_SRC

#+RESULTS:
: ((section-name . (eval (car (org-get-outline-path t)))) (subsection-name . (eval (car (last (org-get-outline-path t))))))


# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (require 'org-inlinetask)
# eval: (require 'org-ref)
# eval: (require 'doi-utils)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (setq ispell-local-dictionary "american")
# eval: (eval (flyspell-mode t))
# eval: (add-to-list 'org-latex-classes '("IEEEtran"
# "\\documentclass{IEEEtran}" ("\\section{%s}" . "\\section*{%s}")
# ("\\subsection{%s}" . "\\subsection*{%s}") ("\\subsubsection{%s}"
# . "\\subsubsection*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}")  ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# End:


