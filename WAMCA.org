# -*- org-export-babel-evaluate: nil -*-
# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+AUTHOR: Otho José Sirtoli Marcondes, Philippe O. A. Navaux, Lucas Mello Schnorr
#+EMAIL: ojsmarcondes@inf.ufrgs.br, navaux@inf.ufrgs.br, schnorr@inf.ufrgs.br
#+DATE: September 2025
#+STARTUP: overview indent
#+LANGUAGE: pt-br
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+BIBLIOGRAPHY: ./refs.bib

#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [conference, 10pt, final]
#+LATEX_HEADER: \usepackage[T1]{fontenc}

* BC                                                               :noexport:
** Compute the distributions

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
def_node_topology <- function(P = 2, Q = 3)
{
  tibble(p = 0:(P-1)) |>
    crossing(tibble(q = 0:(Q-1))) |>
    mutate(Node = 1:(P*Q))
}
def_matrix_topology <- function(M = 16, N = 16)
{
  tibble(X = 0:(M-1)) |>
    crossing(tibble(Y = 0:(N-1)))
}
def_distribution <- function(df.topo, df.matrix) {
  P = df.topo |> distinct(p) |> nrow()
  Q = df.topo |> distinct(q) |> nrow()
  df.matrix |>
    mutate(p = X %% P, q = Y %% Q) |>
    left_join(df.topo, by = join_by(p, q))
}
tribble(~P, ~Q,
        2, 3,
        3, 2,
        1, 6,
        6, 1) |>
  mutate(KEY = paste0(P, "x", Q)) |>
  mutate(TOPO = map2(P, Q, def_node_topology)) |>
  mutate(M = 16, N = 16) |>
  mutate(MATR = map2(M, N, def_matrix_topology)) |>
  mutate(OUT2 = map2(TOPO, MATR, def_distribution)) -> df
#+end_src

#+RESULTS:
** Theme

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
default_theme <- function(base_size = 22, expand = 0.0, legend_title = FALSE, skip_x = FALSE) {
  ret <- list()

  ret[[length(ret) + 1]] <- theme_bw(base_size = base_size)
  ret[[length(ret) + 1]] <- theme(
    plot.margin = unit(c(0, 0, 0, 0), "cm"),
    legend.spacing = unit(3, "cm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0, 0, 0, 0)
  )
  ret[[length(ret) + 1]] <- guides(color = guide_legend(nrow = 1))
  if (!legend_title) {
    ret[[length(ret) + 1]] <- theme(legend.title = element_blank())
  }
  return(ret)
}
#+end_src

#+RESULTS:

** Plot

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
df |>
  select(KEY, OUT2) |>
  unnest(OUT2) |>
  ggplot(aes(x=X,
             xmin=X,
             xmax=X+1,
             y=Y,
             ymin=Y,
             ymax=Y+1,
             fill=as.factor(Node))) +
  scale_fill_brewer(palette = "Set1") +
  geom_rect() +
  default_theme() +
  facet_wrap(~KEY, nrow=1) +
  guides(fill = guide_legend(nrow = 1, override.aes = list(alpha=1))) +
  xlab("X Tile Coordinate") + ylab("Y Tile Coordinate") +
  scale_x_continuous(breaks = seq(0,16, by=3)) +
  scale_y_reverse(breaks = seq(0,16, by=3)) -> p
ggsave("img/bc.pdf", width=13, height=4.5)
#+end_src

#+RESULTS:

* *Paper*                                                              :ignore:
** Frontpage                                                        :ignore:
#+BEGIN_EXPORT latex 
\title{Impact of Data Distribution and Schedulers for the LU Factorization on Multi-Core Clusters}

\author{
\IEEEauthorblockN{Otho José Sirtoli Marcondes\IEEEauthorrefmark{1},
                  Philippe O. A. Navaux\IEEEauthorrefmark{1},
                  Lucas Mello Schnorr\IEEEauthorrefmark{1}}
\IEEEauthorblockN{\IEEEauthorrefmark{1} Institute of Informatics/PPGC/UFRGS, Porto Alegre, Brazil}
}
#+END_EXPORT

#+LaTeX: \maketitle

** TODO Abstract                                                    :ignore:

#+LaTeX: \begin{abstract}
As the demand for more computational resources grows, the usage of clusters has become one of the main options to satisfy this need. In order to exploit these resources efficiently, the distribution of data between nodes must be considered as an important factor in application performance. This study aims to analyze the impact of the static block-cyclic data distribution and different dynamic schedulers of the linear algebra LU factorization on clusters. The analysis focuses on the execution time to explain how the application's behavior is influenced by the data distribution and scheduling strategy.

#+LaTeX: \end{abstract}

** Introduction Plan                                              :noexport:

1. HPC is paramount today, important for so many fields, the more
   frequent platform in this field is clusters equipped with
   multi-core nodes.
2. Dense linear algebra is the basis of so many applications, LU dense
   factorization of large systems of linear equations is part of so
   many applications in so different fields (provide example)
3. To compute the LU factorization in HPC clusters, composed of so
   many nodes, we need to carry out the load balance among the
   machines. Among the many methods that exist, we can use static
   partitioning of the problem, dynamic, and hybrid. Each of them has
   advantages and disatdanvagens. Explain them.
4. The BLock-cyclic static partintionng of the Scalapack package is
   the method that has been used for so much time, it is the de-facto
   method in popularized benchmarks such as the Linpack.
5. Explain how Block-cyclic works, perhaps with an example of 6 nodes
   and a 16x16 tiled matrix. The goal is to use a static partitioning
   that minimizes the communication frontiers thereby reducing
   synchronization costs. Explain the limitation for cases where the
   number of machines are a prime number. This leads to cases where
   not the total number of machines can be used adequately. For
   example, with 6 nodes, there is no ideal solution that truly
   minimizes communications as you can either use 2x3 or 3x2.
6. There exists alternatives for the Block-Cyclic approach such as the
   1D1D, that despite the sophystication and insentitive for the
   number of machines, are much harder to implement for typical pure
   MPI+X cases, because communications need to be take care
   explicitely. Because of this reason, methods such as the 1D1D and
   variants are not widely used.
7. With the ever scalability of large clusters, composed with an
   increasing number of nodes, static partitioning have become an
   permanent issue because of it cannot adapt to runtime variations
   such as heterogeneous node performance, network congestion, or
   varying computational loads that emerge during the factorization
   process.
8. Because of this, hybrid data partitioning have become a modern
   solution. The method combines static partitioning among the nodes
   of the cluster, very frequently done manually by the programmer,
   while dynamic intra-node scheduling automatic heuristics take care
   of tasks attributed to a node. This method has been popularized
   first by runtimes such as StarPU-MPI cite:augonnet2012starpu, and
   then continued in other solutions such as PaRSEC
   cite:bosilca2013parsec, Specx cite:cardosi2025specx, TaskTorrent
   cite:cambier2020tasktorrent, and CHAMELEON
   cite:klinkenberg2020chameleon.
9. This paper focuses on a scenario that combines static data
   partitioning with dynamic task scheduling. By leveraging
   task-based runtimes, we aim to dynamically schedule tasks at
   runtime while maintaining a static block layout of data. This
   approach enables better adaptability to runtime variations, such
   as load imbalance and communication delays, while preserving the
   advantages of a static data map.
10. As a case study, we explore the LU factorization, a fundamental
    operation in linear algebra widely used in scientific
    computing. We adopt a block cyclic distribution scheme for the
    input matrix, a method that balances the computational load and
    spreads data evenly across processes. Our goal is to evaluate how
    dynamic scheduling of tasks can improve the performance of LU
    factorization in clusters.
11. Throughout the development of this work, several challenges were
    encountered related to the use of MPI for executing applications
    across multiple nodes. These included: configuration challenges
    with Guix for package management across distributed nodes; issues
    related to the TCP interface in the MPI NewMadeleine
    implementation; and errors when using StarVZ
    cite:pinto2021providing visualization framework with the traces
    collected from the executions (still not resolved).

** Introduction                                                   :noexport:

The computational demands of modern scientific and engineering
applications continue to grow exponentially, driving the need for
increasingly efficient parallel algorithms on High-Performance
Computing (HPC) systems cite:dongarra2017. These HPC systems plays a
major role in all research and engineering fields today, from the
execution of simulations of large-scale scientific problems to train
Large Language Models (LLMs) for general AI utilization. These systems
consists mainly by computing clusters equipped with multi-core nodes
interconnected by a fast network.

Dense linear algebra operations form the computational backbone of
numerous scientific domains, from climate modeling and computational
fluid dynamics to quantum chemistry and machine learning. Among these
operations, the LU factorization stands as a fundamental building
block, appearing in the solution of linear systems, matrix inversion,
and determinant computation across countless scientific applications
cite:lu2023.

High-Performance Computing (HPC) systems




To maximize application performance
on these types of systems, it is essential to consider both inter-node
communication efficiency and workload balance across the computing
nodes.


To maximize application performance on clusters, it
is essential to consider both inter-node communication efficiency and
workload balance across the computing nodes.

** Introduction                                                   :noexport:

High-Performance Computing (HPC) systems, particularly computing
clusters, are essential for solving large-scale scientific and
engineering problems. These clusters consist of multiple
interconnected nodes, each with its own processor units and memory. In
order 

A crucial aspect of achieving efficient parallel performance is data
partitioning, which determines how data is divided and distributed
across the computing nodes. Among various strategies, static data
partitioning is commonly used due to its simplicity and low runtime
overhead. One of the examples of static data distribution is the
block-cyclic (BC) distribution, a method that was popularized by the
ScaLAPACK cite:blackford1997scalapack library.

This paper focuses on a scenario that combines static data
partitioning with dynamic task scheduling. By leveraging task-based
runtimes, we aim to dynamically schedule tasks at runtime while
maintaining a static block layout of data. This approach enables
better adaptability to runtime variations, such as load imbalance and
communication delays, while preserving the advantages of a static data
map.

As a case study, we explore the LU factorization, a fundamental
operation in linear algebra widely used in scientific computing. We
adopt a block cyclic distribution scheme for the input matrix, a
method that balances the computational load and spreads data evenly
across processes. Our goal is to evaluate how dynamic scheduling of
tasks can improve the performance of LU factorization in clusters.

Throughout the development of this work, several challenges were
encountered related to the use of MPI for executing applications
across multiple nodes. These included: configuration challenges with
Guix for package management across distributed nodes; issues related
to the TCP interface in the MPI NewMadeleine implementation; and
errors when using StarVZ cite:pinto2021providing visualization
framework with the traces collected from the executions (still not
resolved).

The paper is structured as follows. Section [[sec:related]] presents some
related work on matrix distribution and modern task-based
runtimes. Section [[sec:methodology]] details our methodology and explains
how we conducted the experiments in our investigation. Section
[[sec:results]] presents the experiments and their results. Section
[[sec:conclusion]] concludes this work with some considerations.

** Related Work
<<sec:related>>
*** Matrix distribution

ScaLAPACK cite:blackford1997scalapack is the message passing version
of LAPACK cite:anderson1999lapack, and also the standard library for
linear algebra operations over parallel distributed platforms. In this
paper, the LU factorization will be the focus, as its parallelization
strategy is similar to others.

As shown in the Figure [[fig:LU-factor]], the LU factorization of a given
matrix $A$ is defined as $A=LU$, where $L$ is a lower triangular
matrix and $U$ is an upper triangular matrix. The LU algorithm relies
on three different LAPACK kernels: \verb|DGTRF-NOPIV|, \verb|DTRSM|
and \verb|DGEMM|. This application has a tendency to be dominated by
\verb|DGEMM| kernels when $N$ is large, which makes it mandatory to
have well distributed sub-matrixes between the nodes.

#+name: fig:LU-factor
#+caption: The LU algorithm (left) without pivoting, and the regions of A updated at iteration k (right). cite:nesi2020communication
#+attr_latex: :width .5\textwidth
[[./LU-factor.png]]

The block cyclic distribution, popularized by the ScaLAPACK
cite:blackford1997scalapack depends on the $P x Q$ parameters and the
number of available nodes. Based on the P value, the matrix will be
partitioned differently across the nodes. In the Figure [[fig:BC]]
we can visualize that while P is 1 (each panel show a PxQ
distribution), there is only one node per row, as in reverse of the
8x1 distribution, where there is only one node per column. For the 2x4
and 4x2 cases, the distribution is interleaved.

#+name: fig:BC
#+caption: Example of a block cyclic distribution across 8 nodes cite:garcia2018visual
#+attr_latex: :width .5\textwidth
[[./BC.png]]

*** Task-based paradigm

As the computers used in HPC environments became more complex,
adapting and exploiting them to their full potential has become
increasingly challenging. The task-based paradigm was designed to
solve these new challenges. It relies on a DAG (Directed Acyclic
Graph) to represent the relation between tasks and their dependencies
(edges). The scheduler then can dynamically allocate these tasks
during execution time, according to the dependencies of the graph and
the scheduler heuristic cite:faverge2023programming. Chameleon
cite:agullo:inria-00547847, as other linear algebra libraries such as
DPLASMA cite:bosilca2011flexible are built on task-based runtimes,
which allows them to efficiently exploit their computational resources
of clusters. The scheduler heuristics studied in this work are the
following:
- \verb|lws|: stands for locality work stealing. When a worker becomes
  idle, it steals a task from a neighboring worker;
- \verb|random|: tasks are distributed randomly according the assumed
  worker overall performance;
- \verb|dmda|: takes task execution performance models and data
  transfer time into account;
- \verb|dmdas|: same as \verb|dmda|, but also take into account task
  priorities and data buffer availability on the target device.

** TODO Experimental Methodology
<<sec:methodology>>

To enable execution on multiple nodes, we employed StarPU
cite:augonnet2009starpu, which provides a variety of scheduling
policies and built-in support for application tracing.

StarPU is a task-based runtime system for heterogeneous platforms,
being multicore or multinode. The StarPU uses the Sequential Task-Flow
(STF) cite:kennedy2001optimizing, where the tasks are sequentially
submitted to the runtime that is responsible for their
scheduling. Each task can have one implementation for a respective
computational resource (CPU, GPU) called worker, and the scheduler
must assign a task to one of the available workers during the program
execution. To enable multi-node execution, the StarPU-MPI extension
was used cite:augonnet2012starpu.

# NEEDS TO BE REDONE (64 HAD TOO MUCH IDLE TIME WITH TRACES)
We utilized Chameleon cite:agullo:inria-00547847 implementation of the LU factorization, with a matrix size of 16000x16000 block size for all experiments. This value was taken from a preliminary execution only varying the block size as shown in Figure~\ref{fig:timeBlocks}, that depicts different blocks dimensions and their respective execution times. It is possible to observe that the 360 block size had the best performance among the other values.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{block-def.pdf}
\caption{Execution times per block dimension}
\label{fig:timeBlocks}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{block-size.pdf}
\caption{Execution times per block dimension}
\label{fig:block-size}
\end{figure}

# IMAGE OF BLOCK SIZE

The executions were performed in the PCAD from UFRGS using the
\verb|Cei| partition. \verb|Cei| comprises six nodes, each one with
two Intel Xeon Silver 4116 (24 cores/CPU). We used the 1.4.7 StarPU
and 1.3.0 Chameleon version. We also used NewMadeleine
cite:aumage2007new MPI implementation as OpenMPI cite:gabriel2004open
presented significant idle times during the executions. The
NewMadeleine version used was from commit \verb|6e1a64d0| from June
2025, which resolved a TCP interface issue that we reported. For the
execution time evaluation, each execution was run 10 times and the
standard deviation was lower than 5\%.

** TODO Results
<<sec:results>>

Figure~\ref{fig:timePQ} depicts four panels, aligned in the X
dimension (time), each showing the execution time of a different
scheduler (\verb|random|, \verb|lws|, \verb|dmdas|, \verb|dmda|) with
a fixed PxQ configuration. The standard deviation is represented by
the black error bars on each bar. We can see that the \verb|lws| and
\verb|random| schedulers did not present much variation when changing
the PxQ configuration. As for the \verb|dmdas| and \verb|dmda|, both
of them showed significantly better performance when utilizing the
$P=2$ $Q=3$ and $P=3$ $Q=2$ configurations.

# NEEDS TO BE REDONE (DONE WITH THE 64 BLOCK SIZE)
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{tempo_medio_com_desvio_padrao_por_PQ.png}
\caption{Execution times based on the PxQ configuration}
\label{fig:timePQ}
\end{figure}

Figure~\ref{fig:timeSched} depicts four panels, aligned in the X dimension (time), each of them showing the execution time of a PxQ configuration with a fixed scheduler heuristic. The standard deviation is represented by the black error bars on each bar. We can see that the \verb|lws| scheduler had the best results among the schedulers fallowed by the \verb|random| scheduler. The \verb|dmda| and \verb|dmdas| had similar performance, with performance gains when P and Q are interleaved.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{tempo_medio_com_desvio_padrao_por_scheduler.png}
\caption{Execution times based on the scheduler heuristic}
\label{fig:timeSched}
\end{figure}

** More                                                           :noexport:
\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{719424.pdf}
\caption{Trace of an execution of LU-Factor at cei machine (lws heuristic)}
\label{fig:trace}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{719448.pdf}
\caption{Trace of an execution of LU-Factor at cei machine (dmdas heuristic)}
\label{fig:trace}
\end{figure}

** TODO Conclusion
<<sec:conclusion>>

The study examines the impact of data distribution using Block cyclic
and also the impact of different scheduler heuristics in the context
of task-based runtime in clusters. The linear algebra LU factorization
application provided by Chameleon was used as a means to analyze how
these configurations impact performance. The \verb|dmda| and
\verb|dmdas| heuristics presented similar behavior in their execution
times, showing performance gains when the P and Q were
interleaved. The \verb|lws| heuristic presented the best results in
terms of performance, although the P and Q parameters did not have
significant impact in it. The \verb|random| heuristic also showed no
significant impact on its performance based on the P and Q parameters.

# The various issues encountered during the development of this work caused the executions utilizing NewMadeleine implementation of MPI were only conducted toward the end of the available time. Also, to build a stronger argument about why a given scheduler or distribution outperformed the others, the use of execution traces would be necessary. These traces would make possible to visualize the behavior of the application during its execution. As previously mentioned, there are still ongoing problems in the utilization of the FxT traces with the StarVZ framework. The next steps would consist of: resolve the issues preventing StarVZ usage and use SimGrid cite:CASANOVA2025103125 to run simulations and scale the number of nodes.

#+latex: \noindent
*Acknowledgements*.
#+latex: %
The experiments in this work used the PCAD infrastructure,
http://gppd-hpc.inf.ufrgs.br, at INF/UFRGS.  We also acknowledge the
Brazilian National Council for Scientific Technological Development
(CNPq) for their financial scholarship support. This study was
financed in part by the Coordenação de Aperfeiçoamento de Pessoal de
Nível Superior - Brasil (CAPES) - Finance Code 001, the FAPERGS
(16/354-8, 16/348-8), and Petrobras (2020/00182-5).

** References                                                       :ignore:

#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{refs}

* Bibtex                                                           :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib
@article{dongarra2024co,
  title={The co-evolution of computational physics and high-performance computing},
  author={Dongarra, Jack and Keyes, David},
  journal={Nature Reviews Physics},
  volume={6},
  number={10},
  pages={621--627},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@book{blackford1997scalapack,
  title={ScaLAPACK users' guide},
  author={Blackford, L Susan and Choi, Jaeyoung and Cleary, Andy and D'Azevedo, Eduardo and Demmel, James and Dhillon, Inderjit and Dongarra, Jack and Hammarling, Sven and Henry, Greg and Petitet, Antoine and others},
  year={1997},
  publisher={SIAM}
}

@inproceedings{augonnet2009starpu,
  title={StarPU: a unified platform for task scheduling on heterogeneous multicore architectures},
  author={Augonnet, C{\'e}dric and Thibault, Samuel and Namyst, Raymond and Wacrenier, Pierre-Andr{\'e}},
  booktitle={European Conference on Parallel Processing},
  pages={863--874},
  year={2009},
  organization={Springer}
}

@inproceedings{augonnet2012starpu,
  title={StarPU-MPI: Task programming over clusters of machines enhanced with accelerators},
  author={Augonnet, C{\'e}dric and Aumage, Olivier and Furmento, Nathalie and Namyst, Raymond and Thibault, Samuel},
  booktitle={European MPI Users' Group Meeting},
  pages={298--299},
  year={2012},
  organization={Springer}
}

@incollection{agullo:inria-00547847,
  TITLE = {{Faster, Cheaper, Better -- a Hybridization Methodology to Develop Linear Algebra Software for GPUs}},
  AUTHOR = {Agullo, Emmanuel and Augonnet, C{\'e}dric and Dongarra, Jack and Ltaief, Hatem and Namyst, Raymond and Thibault, Samuel and Tomov, Stanimire},
  URL = {https://inria.hal.science/inria-00547847},
  BOOKTITLE = {{GPU Computing Gems}},
  EDITOR = {Wen-mei W. Hwu},
  PUBLISHER = {{Morgan Kaufmann}},
  VOLUME = {2},
  YEAR = {2010},
  MONTH = Sep,
  PDF = {https://inria.hal.science/inria-00547847v1/file/gpucomputinggems_plagma.pdf},
  HAL_ID = {inria-00547847},
  HAL_VERSION = {v1},
}

@book{kennedy2001optimizing,
  title={Optimizing compilers for modern architectures: a dependence-based approach},
  author={Kennedy, Ken and Allen, John R},
  year={2001},
  publisher={Morgan Kaufmann Publishers Inc.}
}

@inproceedings{aumage2007new,
  title={New madeleine: A fast communication scheduling engine for high performance networks},
  author={Aumage, Olivier and Brunet, Elisabeth and Furmento, Nathalie and Namyst, Raymond},
  booktitle={2007 IEEE International Parallel and Distributed Processing Symposium},
  pages={1--8},
  year={2007},
  organization={IEEE}
}
@inproceedings{gabriel2004open,
  title={Open MPI: Goals, concept, and design of a next generation MPI implementation},
  author={Gabriel, Edgar and Fagg, Graham E and Bosilca, George and Angskun, Thara and Dongarra, Jack J and Squyres, Jeffrey M and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and others},
  booktitle={European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting},
  pages={97--104},
  year={2004},
  organization={Springer}
}

@inproceedings{nesi2020communication,
  title={Communication-aware load balancing of the LU factorization over heterogeneous clusters},
  author={Nesi, Lucas Leandro and Schnorr, Lucas Mello and Legrand, Arnaud},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={54--63},
  year={2020},
  organization={IEEE}
}

@article{garcia2018visual,
  title={A visual performance analysis framework for task-based parallel applications running on hybrid clusters},
  author={Garcia Pinto, Vin{\'\i}cius and Mello Schnorr, Lucas and Stanisic, Luka and Legrand, Arnaud and Thibault, Samuel and Danjean, Vincent},
  journal={Concurrency and Computation: Practice and Experience},
  volume={30},
  number={18},
  pages={e4472},
  year={2018},
  publisher={Wiley Online Library}
}

@article{faverge2023programming,
  title={Programming heterogeneous architectures using hierarchical tasks},
  author={Faverge, Mathieu and Furmento, Nathalie and Guermouche, Abdou and Lucas, Gwenol{\'e} and Namyst, Raymond and Thibault, Samuel and Wacrenier, Pierre-andr{\'e}},
  journal={Concurrency and Computation: Practice and Experience},
  volume={35},
  number={25},
  pages={e7811},
  year={2023},
  publisher={Wiley Online Library}
}

@inproceedings{bosilca2011flexible,
  title={Flexible development of dense linear algebra algorithms on massively parallel architectures with DPLASMA},
  author={Bosilca, George and Bouteiller, Aurelien and Danalis, Anthony and Faverge, Mathieu and Haidar, Azzam and Herault, Thomas and Kurzak, Jakub and Langou, Julien and Lemarinier, Pierre and Ltaief, Hatem and others},
  booktitle={2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
  pages={1432--1441},
  year={2011},
  organization={IEEE}
}

@book{anderson1999lapack,
  title={LAPACK users' guide},
  author={Anderson, Edward and Bai, Zhaojun and Bischof, Christian and Blackford, L Susan and Demmel, James and Dongarra, Jack and Du Croz, Jeremy and Greenbaum, Anne and Hammarling, Sven and McKenney, Alan and others},
  year={1999},
  publisher={SIAM}
}

@inproceedings{pinto2021providing,
  title={Providing in-depth performance analysis for heterogeneous task-based applications with starvz},
  author={Pinto, Vin{\'\i}cius Garcia and Nesi, Lucas Leandro and Miletto, Marcelo Cogo and Schnorr, Lucas Mello},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={16--25},
  year={2021},
  organization={IEEE}
}

@article{CASANOVA2025103125,
  title = {{Lowering entry barriers to developing custom simulators of distributed applications and platforms with SimGrid}},
  journal = {Parallel Computing},
  volume = {123},
  pages = {103-125},
  year = {2025},
  issn = {0167-8191},
  doi = {https://doi.org/10.1016/j.parco.2025.103125},
  author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr{\'e}d{\'e}ric},
  keywords = {Simulation of distributed computing systems, SimGrid},
  pdf = {https://hal.science/hal-04909441/file/paper.pdf}
}


@ARTICLE{dongarra2017,
  author={Dongarra, Jack and Tomov, Stanimire and Luszczek, Piotr and Kurzak, Jakub and Gates, Mark and Yamazaki, Ichitaro and Anzt, Hartwig and Haidar, Azzam and Abdelfattah, Ahmad},
  journal={Computing in Science \& Engineering},
  title={With Extreme Computing, the Rules Have Changed},
  year={2017},
  volume={19},
  number={3},
  pages={52-62},
  keywords={Program processors;High performance computing;Symmetric matrices;Parallel processing;Computational modeling;Market research;High-performance computing;exascale;algorithms;scheduling;autotuning;scientific computing},
  doi={10.1109/MCSE.2017.48}
}

@inproceedings{lu2023,
author = {Xia, Yang and Jiang, Peng and Agrawal, Gagan and Ramnath, Rajiv},
title = {End-to-End LU Factorization of Large Matrices on GPUs},
year = {2023},
isbn = {9798400700156},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3572848.3577486},
doi = {10.1145/3572848.3577486},
abstract = {LU factorization for sparse matrices is an important computing step for many engineering and scientific problems such as circuit simulation. There have been many efforts toward parallelizing and scaling this algorithm, which include the recent efforts targeting the GPUs. However, it is still challenging to deploy a complete sparse LU factorization workflow on a GPU due to high memory requirements and data dependencies. In this paper, we propose the first complete GPU solution for sparse LU factorization. To achieve this goal, we propose an out-of-core implementation of the symbolic execution phase, thus removing the bottleneck due to large intermediate data structures. Next, we propose a dynamic parallelism implementation of Kahn's algorithm for topological sort on the GPUs. Finally, for the numeric factorization phase, we increase the parallelism degree by removing the memory limits for large matrices as compared to the existing implementation approaches. Experimental results show that compared with an implementation modified from GLU 3.0, our out-of-core version achieves speedups of 1.13--32.65X. Further, our out-of-core implementation achieves a speedup of 1.2--2.2 over an optimized unified memory implementation on the GPU. Finally, we show that the optimizations we introduce for numeric factorization turn out to be effective.},
booktitle = {Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming},
pages = {288–300},
numpages = {13},
keywords = {GPU acceleration, LU factorization, memory limits},
location = {Montreal, QC, Canada},
series = {PPoPP '23}
}

@inproceedings{augonnet2012starpu,
  title={StarPU-MPI: Task programming over clusters of machines enhanced with accelerators},
  author={Augonnet, C{\'e}dric and Aumage, Olivier and Furmento, Nathalie and Namyst, Raymond and Thibault, Samuel},
  booktitle={European MPI Users' Group Meeting},
  pages={298--299},
  year={2012},
  organization={Springer}
}

#+end_src
* Emacs setup                                                      :noexport:

#+BEGIN_SRC elisp
(setq org-export-global-macros
      '((section-name . "(eval (car (org-get-outline-path t)))")
        (subsection-name . "(eval (car (last (org-get-outline-path t))))")))
#+END_SRC

#+RESULTS:
: ((section-name . (eval (car (org-get-outline-path t)))) (subsection-name . (eval (car (last (org-get-outline-path t))))))


# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (require 'org-inlinetask)
# eval: (require 'org-ref)
# eval: (require 'doi-utils)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (setq ispell-local-dictionary "american")
# eval: (eval (flyspell-mode t))
# eval: (add-to-list 'org-latex-classes '("IEEEtran"
# "\\documentclass{IEEEtran}" ("\\section{%s}" . "\\section*{%s}")
# ("\\subsection{%s}" . "\\subsection*{%s}") ("\\subsubsection{%s}"
# . "\\subsubsection*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}")  ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# End:


