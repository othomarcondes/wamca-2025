# -*- org-export-babel-evaluate: nil -*-
# -*- coding: utf-8 -*-
# -*- mode: org -*-
#+TITLE: LU-factor
#+AUTHOR: Otho José Sirtoli Marcondes, Lucas Mello Schnorr
#+EMAIL: ojsmarcondes@inf.ufrgs.br, schnorr@inf.ufrgs.br
#+DATE: September 5, 2018
#+STARTUP: overview indent
#+LANGUAGE: pt-br
#+OPTIONS: H:3 creator:nil timestamp:nil skip:nil toc:nil num:t ^:nil ~:~
#+OPTIONS: author:nil title:nil date:nil
#+TAGS: noexport(n) deprecated(d) ignore(i)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+BIBLIOGRAPHY: ./refs.bib

#+LATEX_CLASS: IEEEtran
#+LATEX_CLASS_OPTIONS: [conference, 10pt, final]
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{breakurl}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usepackage[font=footnotesize]{subfig}
#+LATEX_HEADER: \newcommand{\prettysmall}{\fontsize{4}{6}\selectfont}
#+LATEX_HEADER: \newcommand{\pstwo}{\fontsize{8}{8}\selectfont}
#+LATEX_HEADER: \usepackage{color,colortbl,xcolor}
#+LATEX_HEADER: \definecolor{dgeqrtC}{HTML}{e41a1c}
#+LATEX_HEADER: \definecolor{dlarfbC}{HTML}{377eb8}
#+LATEX_HEADER: \definecolor{dtpqrtC}{HTML}{4daf4a}
#+LATEX_HEADER: \definecolor{dtpmqrtC}{HTML}{984ea3}
#+LATEX_HEADER: \newcommand{\dgeqrtcolor}{red}
#+LATEX_HEADER: \newcommand{\dlarfbcolor}{blue}
#+LATEX_HEADER: \newcommand{\dtpqrtcolor}{green}
#+LATEX_HEADER: \newcommand{\dtpmqrtcolor}{purple}
#+LATEX_HEADER: \lstdefinestyle{customc}{morekeywords={DGEQRT },keywordstyle=\color{dgeqrtC},morekeywords=[2]{DLARFB},keywordstyle=[2]\color{dlarfbC},morekeywords=[3]{DTPQRT},keywordstyle=[3]\color{dtpqrtC},morekeywords=[4]{DTPMQRT},keywordstyle=[4]\color{dtpmqrtC}, numbers=left, breakatwhitespace=false,        breaklines=true,captionpos=b,keepspaces=true,numbersep=5pt,showspaces=false,showstringspaces=false,showtabs=false,tabsize=2}
#+LATEX_HEADER: \lstset{ basicstyle=\ttfamily\small, breaklines=true, columns=fullflexible,numbers=left,numberstyle=\tiny\color{gray}, xleftmargin=10pt,flexiblecolumns=false}

#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{caption}
#+LATEX_HEADER: \usepackage{subcaption}
#+LATEX_HEADER: \usepackage{tikz}
#+LATEX_HEADER: \usetikzlibrary{automata,arrows,positioning,calc}

#+LATEX_HEADER: \usepackage{subfig}

#+LATEX_HEADER: \newcommand{\subfigureautorefname}{Figure} % allows autoref to find subimages.

* TODOS :noexport:
- [ ] Abstract
- [x] Conclusion
- [ ] Use StarVZ/OpenMP DAG
- [x] Put the hyperthreading experiment setup
- [x] Make the code snip better
- [x] Adds the OpenMP wihtout hyperthreading data
  - [x] Initial analisys
- [x] Fix charts
- [x] Ajustar a introdução conforme o Lucas pediu
- [x] Adds prio OpenMP
  - [x] Run the experiments again
- [x] Redo the StarPU schedulers description
- [x] Introduction
  - [x] Representative cases
  - [x] Shows one case where hyperthreading is better for dense linear algebra
  - [x] Presents an idea for a priority equation for the tile qr factorization
* *Paper* :ignore:
** Latex configurations                                             :ignore:

#+BEGIN_EXPORT latex
#+END_EXPORT

** Frontpage                                                        :ignore:
#+BEGIN_EXPORT latex 
\title{Impact of Data Distribution and Schedulers for LU Factorization on Clusters}
% Let's keep the next title for the official post-WSPPD publication
% Visual Performance Analysis of Memory Operations in Heterogeneous Task-Based Applications}

\author{
\IEEEauthorblockN{Otho José Sirtoli Marcondes\IEEEauthorrefmark{1},
                  Lucas Mello Schnorr\IEEEauthorrefmark{1}}
\IEEEauthorblockN{\IEEEauthorrefmark{1} Institute of Informatics/PPGC/UFRGS, Porto Alegre, Brazil}
}
#+END_EXPORT

#+LaTeX: \maketitle

\newcommand{\kiter}{K Iteration}

** TODO Abstract :ignore:

#+LaTeX: \begin{abstract}
As the demand for more computational resources grows, the usage of clusters has become one of the main options to satisfy this need. In order to exploit these resources efficiently, the distribution of data between nodes must be considered as an important factor in application performance. This study aims to analyze the impact of the static block-cyclic data distribution and different dynamic schedulers of the linear algebra LU factorization on clusters. The analysis focuses on the execution time to explain how the application's behavior is influenced by the data distribution and scheduling strategy.

#+LaTeX: \end{abstract}

** Introduction
High-Performance Computing (HPC) systems, particularly computing clusters, are essential for solving large-scale scientific and engineering problems. These clusters consist of multiple interconnected nodes, each with its own processor units and memory. In order to maximize application performance on clusters, it is essential to consider both inter-node communication efficiency and workload balance across the computing nodes.

A crucial aspect of achieving efficient parallel performance is data partitioning, which determines how data is divided and distributed across the computing nodes. Among various strategies, static data partitioning is commonly used due to its simplicity and low runtime overhead. One of the examples of static data distribution is the block-cyclic (BC) distribution, a method that was popularized by the ScaLAPACK \cite{blackford1997scalapack} library.

This paper focuses on a scenario that combines static data partitioning with dynamic task scheduling. By leveraging task-based runtimes, we aim to dynamically schedule tasks at runtime while maintaining a static block layout of data. This approach enables better adaptability to runtime variations, such as load imbalance and communication delays, while preserving the advantages of a static data map.

As a case study, we explore the LU factorization, a fundamental operation in linear algebra widely used in scientific computing. We adopt a block cyclic distribution scheme for the input matrix, a method that balances the computational load and spreads data evenly across processes. Our goal is to evaluate how dynamic scheduling of tasks can improve the performance of LU factorization in clusters.

Throughout the development of this work, several challenges were encountered related to the use of MPI for executing applications across multiple nodes. These included: configuration challenges with Guix for package management across distributed nodes; issues related to the TCP interface in the MPI NewMadeleine implementation; and errors when using StarVZ \cite{pinto2021providing} visualization framework with the traces collected from the executions (still not resolved).

The paper is structured as follows. Section~\ref{sec:related} presents some related work on matrix distribution and modern task-based runtimes. Section~\ref{sec:methodology} details our methodology and explains how we conducted the experiments in our investigation. Section~\ref{sec:results} presents the experiments and their results. Section~\ref{sec:conclusion} concludes this work with some considerations.

** Related Work
:PROPERTIES:
:CUSTOM_ID: sec:related
:END:
*** Matrix distribution
ScaLAPACK \cite{blackford1997scalapack} is the message passing version of LAPACK \cite{anderson1999lapack}, and also the standard library for linear algebra operations over parallel distributed platforms. In this paper, the LU factorization will be the focus, as its parallelization strategy is similar to others.

 As shown in the Figure~\ref{fig:LU-factor}, the LU factorization of a given matrix $A$ is defined as $A=LU$, where $L$ is a lower triangular matrix and $U$ is an upper triangular matrix. The LU algorithm relies on three different LAPACK kernels: \verb|DGTRF-NOPIV|, \verb|DTRSM| and \verb|DGEMM|. This application has a tendency to be dominated by \verb|DGEMM| kernels when $N$ is large, which makes it mandatory to have well distributed sub-matrixes between the nodes.

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{LU-factor.png}
\caption{The LU algorithm (left) without pivoting, and the regions of A updated at iteration k (right). \cite{nesi2020communication}}
\label{fig:LU-factor}
\end{figure}

The block cyclic distribution, popularized by the ScaLAPACK \cite{blackford1997scalapack} depends on the $P x Q$ parameters and the number of available nodes. Based on the P value, the matrix will be partitioned differently across the nodes. In the Figure~\ref{fig:BC} we can visualize that while P is 1 (each panel show a PxQ distribution), there is only one node per row, as in reverse of the 8x1 distribution, where there is only one node per column. For the 2x4 and 4x2 cases, the distribution is interleaved.

\begin{figure}[ht]
\centering
\includegraphics[width=.5\textwidth]{BC.png}
\caption{ Example of a block cyclic distribution across 8 nodes \cite{garcia2018visual}}
\label{fig:BC}
\end{figure}


*** Task-based paradigm

As the computers used in HPC environments became more complex, adapting and exploiting them to their full potential has become increasingly challenging. The task-based paradigm was designed to solve these new challenges. It relies on a DAG (Directed Acyclic Graph) to represent the relation between tasks and their dependencies (edges). The scheduler then can dynamically allocate these tasks during execution time, according to the dependencies of the graph and the scheduler heuristic \cite{faverge2023programming}. Chameleon \cite{agullo:inria-00547847}, as other linear algebra libraries such as DPLASMA \cite{bosilca2011flexible} are built on task-based runtimes, which allows them to efficiently exploit their computational resources of clusters. The scheduler heuristics studied in this work are the following:

\begin{itemize}
\item \verb|lws|: stands for locality work stealing. When a worker becomes idle, it steals a task from a neighboring worker;

\item \verb|random|: tasks are distributed randomly according the assumed worker overall performance;

\item \verb|dmda|: takes task execution performance models and data transfer time into account;

\item \verb|dmdas|: same as \verb|dmda|, but also take into account task priorities and data buffer availability on the target device.

\end{itemize}
** TODO Experimental Methodology
:PROPERTIES:
:CUSTOM_ID: sec:methodology
:END:
To enable execution on multiple nodes, we employed StarPU \cite{augonnet2009starpu}, which provides a variety of scheduling policies and built-in support for application tracing.

StarPU is a task-based runtime system for heterogeneous platforms, being multicore or multinode. The StarPU uses the Sequential Task-Flow (STF) \cite{kennedy2001optimizing}, where the tasks are sequentially submitted to the runtime that is responsible for their scheduling. Each task can have one implementation for a respective computational resource (CPU, GPU) called worker, and the scheduler must assign a task to one of the available workers during the program execution. To enable multi-node execution, the StarPU-MPI extension was used \cite{augonnet2012starpu}.

# NEEDS TO BE REDONE (64 HAD TOO MUCH IDLE TIME WITH TRACES)
We utilized Chameleon \cite{agullo:inria-00547847} implementation of the LU factorization, with 360x360 block size for all experiments. This value was taken from a preliminary execution only varying the block size as shown in Figure~\ref{fig:timeBlocks}, that depicts different blocks dimensions and their respective execution times. It is possible to observe that the 360 block size had the best performance among the other values.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{block-def.pdf}
\caption{Execution times per block dimension}
\label{fig:timeBlocks}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{block-size.pdf}
\caption{Execution times per block dimension}
\label{fig:block-size}
\end{figure}


# IMAGE OF BLOCK SIZE

The executions were performed in the PCAD from UFRGS using the \verb|Cei| partition. \verb|Cei| comprises six nodes, each one with two Intel Xeon Silver 4116 (24 cores/CPU). We used the 1.4.7 StarPU and 1.3.0 Chameleon version. We also used NewMadeleine \cite{aumage2007new} MPI implementation as OpenMPI \cite{gabriel2004open} presented significant idle times during the executions. The NewMadeleine version used was from commit \verb|6e1a64d0| from June 2025, which resolved a TCP interface issue that we reported. For the execution time evaluation, each execution was run 10 times and the standard deviation was lower than 5\%.

** TODO Results
:PROPERTIES:
:CUSTOM_ID: sec:results
:END:
Figure~\ref{fig:timePQ} depicts four panels, aligned in the X dimension (time), each showing the execution time of a different scheduler (\verb|random|, \verb|lws|, \verb|dmdas|, \verb|dmda|) with a fixed PxQ configuration. The standard deviation is represented by the black error bars on each bar. We can see that the \verb|lws| and \verb|random| schedulers did not present much variation when changing the PxQ configuration. As for the \verb|dmdas| and \verb|dmda|, both of them showed significantly better performance when utilizing the $P=2$ $Q=3$ and $P=3$ $Q=2$ configurations.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{tempo_medio_com_desvio_padrao_por_PQ.png}
\caption{Execution times based on the PxQ configuration}
\label{fig:timePQ}
\end{figure}

Figure~\ref{fig:timeSched} depicts four panels, aligned in the X dimension (time), each of them showing the execution time of a PxQ configuration with a fixed scheduler heuristic. The standard deviation is represented by the black error bars on each bar. We can see that the \verb|lws| scheduler had the best results among the schedulers fallowed by the \verb|random| scheduler. The \verb|dmda| and \verb|dmdas| had similar performance, with performance gains when P and Q are interleaved.

\begin{figure}[ht]
\centering
\includegraphics[width=0.5\textwidth]{tempo_medio_com_desvio_padrao_por_scheduler.png}
\caption{Execution times based on the scheduler heuristic}
\label{fig:timeSched}
\end{figure}

\begin{figure}[ht]
\centering
\includegraphics[width=0.3\textwidth]{719424.pdf}
\caption{Trace of an execution of LU-Factor at cei machine}
\label{fig:trace}
\end{figure}


** TODO Conclusion
:PROPERTIES:
:CUSTOM_ID: sec:conclusion
:END:
The study examines the impact of data distribution using Block cyclic and also the impact of different scheduler heuristics in the context of task-based runtime in clusters. The linear algebra LU factorization application provided by Chameleon was used as a means to analyze how these configurations impact performance. The \verb|dmda| and \verb|dmdas| heuristics presented similar behavior in their execution times, showing performance gains when the P and Q were interleaved. The \verb|lws| heuristic presented the best results in terms of performance, although the P and Q parameters did not have significant impact in it. The \verb|random| heuristic also showed no significant impact on its performance based on the P and Q parameters.

The various issues encountered during the development of this work caused the executions utilizing NewMadeleine implementation of MPI were only conducted toward the end of the available time. Also, to build a stronger argument about why a given scheduler or distribution outperformed the others, the use of execution traces would be necessary. These traces would make possible to visualize the behavior of the application during its execution. As previously mentioned, there are still ongoing problems in the utilization of the FxT traces with the StarVZ framework. The next steps would consist of: resolve the issues preventing StarVZ usage and use SimGrid \cite{CASANOVA2025103125} to run simulations and scale the number of nodes.
** Acknowledgments :ignore:

#+LATEX:\section*{Acknowledgements}

The experiments in this work used the PCAD infrastructure, http://gppd-hpc.inf.ufrgs.br, at INF/UFRGS.


** References                                                        :ignore:

\clearpage
# See next section to understand how refs.bib file is created.
#+LATEX: \bibliographystyle{IEEEtran}
#+LATEX: \bibliography{refs}

* Charts :noexport:
#+NAME: fig:qr-dag
#+CAPTION: DAG
#+BEGIN_SRC R :exports none :results graphics file :file img.pdf
hist(rnorm(100))
print("hello")
#+END_SRC

#+RESULTS: fig:qr-dag
[[file:img.pdf]]

* Bibtex                                                           :noexport:

Tangle this file with C-c C-v t

#+begin_src bib :tangle refs.bib
@book{blackford1997scalapack,
  title={ScaLAPACK users' guide},
  author={Blackford, L Susan and Choi, Jaeyoung and Cleary, Andy and D'Azevedo, Eduardo and Demmel, James and Dhillon, Inderjit and Dongarra, Jack and Hammarling, Sven and Henry, Greg and Petitet, Antoine and others},
  year={1997},
  publisher={SIAM}
}

@inproceedings{augonnet2009starpu,
  title={StarPU: a unified platform for task scheduling on heterogeneous multicore architectures},
  author={Augonnet, C{\'e}dric and Thibault, Samuel and Namyst, Raymond and Wacrenier, Pierre-Andr{\'e}},
  booktitle={European Conference on Parallel Processing},
  pages={863--874},
  year={2009},
  organization={Springer}
}

@inproceedings{augonnet2012starpu,
  title={StarPU-MPI: Task programming over clusters of machines enhanced with accelerators},
  author={Augonnet, C{\'e}dric and Aumage, Olivier and Furmento, Nathalie and Namyst, Raymond and Thibault, Samuel},
  booktitle={European MPI Users' Group Meeting},
  pages={298--299},
  year={2012},
  organization={Springer}
}

@incollection{agullo:inria-00547847,
  TITLE = {{Faster, Cheaper, Better -- a Hybridization Methodology to Develop Linear Algebra Software for GPUs}},
  AUTHOR = {Agullo, Emmanuel and Augonnet, C{\'e}dric and Dongarra, Jack and Ltaief, Hatem and Namyst, Raymond and Thibault, Samuel and Tomov, Stanimire},
  URL = {https://inria.hal.science/inria-00547847},
  BOOKTITLE = {{GPU Computing Gems}},
  EDITOR = {Wen-mei W. Hwu},
  PUBLISHER = {{Morgan Kaufmann}},
  VOLUME = {2},
  YEAR = {2010},
  MONTH = Sep,
  PDF = {https://inria.hal.science/inria-00547847v1/file/gpucomputinggems_plagma.pdf},
  HAL_ID = {inria-00547847},
  HAL_VERSION = {v1},
}

@book{kennedy2001optimizing,
  title={Optimizing compilers for modern architectures: a dependence-based approach},
  author={Kennedy, Ken and Allen, John R},
  year={2001},
  publisher={Morgan Kaufmann Publishers Inc.}
}

@inproceedings{aumage2007new,
  title={New madeleine: A fast communication scheduling engine for high performance networks},
  author={Aumage, Olivier and Brunet, Elisabeth and Furmento, Nathalie and Namyst, Raymond},
  booktitle={2007 IEEE International Parallel and Distributed Processing Symposium},
  pages={1--8},
  year={2007},
  organization={IEEE}
}
@inproceedings{gabriel2004open,
  title={Open MPI: Goals, concept, and design of a next generation MPI implementation},
  author={Gabriel, Edgar and Fagg, Graham E and Bosilca, George and Angskun, Thara and Dongarra, Jack J and Squyres, Jeffrey M and Sahay, Vishal and Kambadur, Prabhanjan and Barrett, Brian and Lumsdaine, Andrew and others},
  booktitle={European Parallel Virtual Machine/Message Passing Interface Users’ Group Meeting},
  pages={97--104},
  year={2004},
  organization={Springer}
}

@inproceedings{nesi2020communication,
  title={Communication-aware load balancing of the LU factorization over heterogeneous clusters},
  author={Nesi, Lucas Leandro and Schnorr, Lucas Mello and Legrand, Arnaud},
  booktitle={2020 IEEE 26th International Conference on Parallel and Distributed Systems (ICPADS)},
  pages={54--63},
  year={2020},
  organization={IEEE}
}

@article{garcia2018visual,
  title={A visual performance analysis framework for task-based parallel applications running on hybrid clusters},
  author={Garcia Pinto, Vin{\'\i}cius and Mello Schnorr, Lucas and Stanisic, Luka and Legrand, Arnaud and Thibault, Samuel and Danjean, Vincent},
  journal={Concurrency and Computation: Practice and Experience},
  volume={30},
  number={18},
  pages={e4472},
  year={2018},
  publisher={Wiley Online Library}
}

@article{faverge2023programming,
  title={Programming heterogeneous architectures using hierarchical tasks},
  author={Faverge, Mathieu and Furmento, Nathalie and Guermouche, Abdou and Lucas, Gwenol{\'e} and Namyst, Raymond and Thibault, Samuel and Wacrenier, Pierre-andr{\'e}},
  journal={Concurrency and Computation: Practice and Experience},
  volume={35},
  number={25},
  pages={e7811},
  year={2023},
  publisher={Wiley Online Library}
}

@inproceedings{bosilca2011flexible,
  title={Flexible development of dense linear algebra algorithms on massively parallel architectures with DPLASMA},
  author={Bosilca, George and Bouteiller, Aurelien and Danalis, Anthony and Faverge, Mathieu and Haidar, Azzam and Herault, Thomas and Kurzak, Jakub and Langou, Julien and Lemarinier, Pierre and Ltaief, Hatem and others},
  booktitle={2011 IEEE International Symposium on Parallel and Distributed Processing Workshops and Phd Forum},
  pages={1432--1441},
  year={2011},
  organization={IEEE}
}

@book{anderson1999lapack,
  title={LAPACK users' guide},
  author={Anderson, Edward and Bai, Zhaojun and Bischof, Christian and Blackford, L Susan and Demmel, James and Dongarra, Jack and Du Croz, Jeremy and Greenbaum, Anne and Hammarling, Sven and McKenney, Alan and others},
  year={1999},
  publisher={SIAM}
}

@inproceedings{pinto2021providing,
  title={Providing in-depth performance analysis for heterogeneous task-based applications with starvz},
  author={Pinto, Vin{\'\i}cius Garcia and Nesi, Lucas Leandro and Miletto, Marcelo Cogo and Schnorr, Lucas Mello},
  booktitle={2021 IEEE International Parallel and Distributed Processing Symposium Workshops (IPDPSW)},
  pages={16--25},
  year={2021},
  organization={IEEE}
}

@article{CASANOVA2025103125,
  title = {{Lowering entry barriers to developing custom simulators of distributed applications and platforms with SimGrid}},
  journal = {Parallel Computing},
  volume = {123},
  pages = {103-125},
  year = {2025},
  issn = {0167-8191},
  doi = {https://doi.org/10.1016/j.parco.2025.103125},
  author = {Casanova, Henri and Giersch, Arnaud and Legrand, Arnaud and Quinson, Martin and Suter, Fr{\'e}d{\'e}ric},
  keywords = {Simulation of distributed computing systems, SimGrid},
  pdf = {https://hal.science/hal-04909441/file/paper.pdf}
}
#+end_src
* Emacs setup                                                      :noexport:

#+BEGIN_SRC elisp
(setq org-export-global-macros
      '((section-name . "(eval (car (org-get-outline-path t)))")
        (subsection-name . "(eval (car (last (org-get-outline-path t))))")))
#+END_SRC

#+RESULTS:
: ((section-name . (eval (car (org-get-outline-path t)))) (subsection-name . (eval (car (last (org-get-outline-path t))))))


# Local Variables:
# eval: (add-to-list 'load-path ".")
# eval: (require 'ox-extra)
# eval: (require 'org-inlinetask)
# eval: (ox-extras-activate '(ignore-headlines))
# eval: (setq ispell-local-dictionary "american")
# eval: (eval (flyspell-mode t))
# eval: (add-to-list 'org-latex-classes '("IEEEtran"
# "\\documentclass{IEEEtran}" ("\\section{%s}" . "\\section*{%s}")
# ("\\subsection{%s}" . "\\subsection*{%s}") ("\\subsubsection{%s}"
# . "\\subsubsection*{%s}") ("\\paragraph{%s}" . "\\paragraph*{%s}")  ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# End:


