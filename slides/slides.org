# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+startup: beamer
#+STARTUP: overview
#+STARTUP: indent
#+TAGS: noexport(n)

#+TITLE: Impact of Data Distribution and Schedulers for the @@latex: \linebreak@@ LU Factorization on Multi-Core Clusters

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [14pt,xcolor=dvipsnames,presentation,aspectratio=169]
#+OPTIONS:   H:1 num:t toc:nil \n:nil @:t ::t |:t ^:nil -:t f:t *:t <:t title:nil
#+LATEX_HEADER: \usedescriptionitemofwidthas{bl}
#+LATEX_HEADER: \usepackage{ifthen,figlatex,amsmath,amstext,xspace}
#+LATEX_HEADER: \usepackage{boxedminipage,xspace,multicol}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{fancyvrb}
#+LATEX_HEADER: \usetheme{Madrid}
#+LATEX_HEADER: \usecolortheme[named=BrickRed]{structure}
#+LATEX_HEADER:  %\usepackage[colorlinks=true,citecolor=pdfcitecolor,urlcolor=pdfurlcolor,linkcolor=pdflinkcolor,pdfborder={0 0 0}]{hyperref}
#+LATEX_HEADER: \usepackage[round-precision=3,round-mode=figures,scientific-notation=true]{siunitx}
#+LATEX_HEADER: \setbeamertemplate{footline}[frame number]
#+LATEX_HEADER: \setbeamertemplate{navigation symbols}{}
#+LATEX_HEADER: \usepackage{DejaVuSansMono}
#+LATEX_HEADER: %\AtBeginDocument{
#+LATEX_HEADER: %  \definecolor{pdfurlcolor}{rgb}{0,0,0.6}
#+LATEX_HEADER: %  \definecolor{pdfcitecolor}{rgb}{0,0.6,0}
#+LATEX_HEADER: %  \definecolor{pdflinkcolor}{rgb}{0.6,0,0}
#+LATEX_HEADER: %  \definecolor{light}{gray}{.85}
#+LATEX_HEADER: %  \definecolor{vlight}{gray}{.95}
#+LATEX_HEADER: %}
#+LATEX_HEADER: \usepackage{appendixnumberbeamer}
#+LATEX_HEADER: \usepackage{relsize}
#+LATEX_HEADER: \usepackage{color,colortbl}
#+LATEX_HEADER: \definecolor{gray98}{rgb}{0.98,0.98,0.98}
#+LATEX_HEADER: \definecolor{gray20}{rgb}{0.20,0.20,0.20}
#+LATEX_HEADER: \definecolor{gray25}{rgb}{0.25,0.25,0.25}
#+LATEX_HEADER: \definecolor{gray16}{rgb}{0.161,0.161,0.161}
#+LATEX_HEADER: \definecolor{gray60}{rgb}{0.6,0.6,0.6}
#+LATEX_HEADER: \definecolor{gray30}{rgb}{0.3,0.3,0.3}
#+LATEX_HEADER: \definecolor{bgray}{RGB}{248, 248, 248}
#+LATEX_HEADER: \definecolor{amgreen}{RGB}{77, 175, 74}
#+LATEX_HEADER: \definecolor{amblu}{RGB}{55, 126, 184}
#+LATEX_HEADER: \definecolor{amred}{RGB}{228,26,28}
#+LATEX_HEADER: \newcommand{\prettysmall}{\fontsize{6}{8}\selectfont}
#+LATEX_HEADER: \newcommand{\quitesmall}{\fontsize{8}{10}\selectfont}

#+LATEX_HEADER: \usepackage{tikzsymbols}
#+LATEX_HEADER: \def\smiley{\Smiley[1][green!80!white]}
#+LATEX_HEADER: \def\frowny{\Sadey[1][red!80!white]}
#+LATEX_HEADER: \def\winkey{\Winkey[1][yellow]}
#+LATEX_HEADER: \def\smileyitem{\setbeamertemplate{itemize item}{\scriptsize\raise1.25pt\hbox{\donotcoloroutermaths\color{black}$\smiley$}}}
#+LATEX_HEADER: \def\frownyitem{\setbeamertemplate{itemize item}{\scriptsize\raise1.25pt\hbox{\donotcoloroutermaths\color{black}$\frowny$}}}
#+LATEX_HEADER: \def\restoreitem{\setbeamertemplate{itemize item}[ball]}
#+LATEX_HEADER: \def\smileysubitem{\setbeamertemplate{itemize subitem}{\scriptsize\raise1.25pt\hbox{\donotcoloroutermaths\color{black}$\smiley$}}}
#+LATEX_HEADER: \def\frownysubitem{\setbeamertemplate{itemize subitem}{\scriptsize\raise1.25pt\hbox{\donotcoloroutermaths\color{black}$\frowny$}}}
#+LATEX_HEADER: \def\restoresubitem{\setbeamertemplate{itemize subitem}[ball]}

#+LATEX_HEADER: \setbeamerfont{title}{size=\normalsize}

#+LaTeX: \urlstyle{sf}
#+LaTeX: \let\alert=\structure
#+LaTeX: \let\epsilon=\varepsilon
#+LaTeX: \let\leq=\leqslant
#+LaTeX: \let\geq=\geqslant 

#+BEGIN_EXPORT LaTeX
{%\setbeamertemplate{footline}{} 

\author{Otho José Sirtoli Marcondes, Lucas M. Schnorr, Philippe O. A. Navaux \newline Instituto de Informática, UFRGS}
\date{16th Workshop on Applications for Multi-Core Architectures \\
October 28th, 2025 -- Bonito, MS -- Brazil}

\titlegraphic{\vspace{-.2cm
    \includegraphics[scale=0.12]{./logo/ppgc.png}\hspace{2cm}
    \includegraphics[scale=1.6]{./logo/ufrgs.png}}}

\maketitle

#+END_EXPORT

* Context
- HPC is the backbone for groundbreaking research and innovation across numerous scientific and engineering disciplines.
- Modern clusters have thousands of multi-core nodes.
- Dense linear algebra (e.g., LU factorization) is a core workload.

* Introduction and Motivation
Load balancing across nodes is a critical challenge.
- Static data distribution: excellent data locality but lacks adaptability.
- Dynamic scheduling: great adaptability but incurs overhead.
- Hybrid strategies aim to combine both efficiently (StarPU-MPI and CHAMELEON).
Objective
- Analyze how *data distribution* and *scheduling heuristics* affect LU factorization performance.

* Related Work: Static Data Distribution
- *Block-Cyclic (BC)* used in ScaLAPACK, HPL benchmark.
- Balances computation and communication via P×Q grid.
- Limitations with prime node counts or heterogeneous resources.
#+attr_latex: :center no :height 2.5cm
[[../img/bc.pdf]]

* Related Work: Task-Based Runtime Systems
- Frameworks: StarPU, PaRSEC, OmpSs.
- Express computations as task DAGs.
- Adaptability and performance portability.
- Schedulers dynamically assign tasks.

* Experimental Setup: Hardware Environment
| Resource | Specification |
|-----------+---------------|
| Cluster | PCAD @ INF/UFRGS |
| Nodes | 6 |
| Cores per node | 24 (2×12 Xeon Silver 4116) |
| Memory per node | 96 GB DDR4 |
| Network | 10G Ethernet (X540-AT2) |

* Experimental Setup: Software Environment
- CHAMELEON 1.3.0 for dense linear algebra.
- StarPU-MPI 1.4.7 runtime system.
- OpenMPI 4 transport layer.
- GNU Guix for reproducible package management.
- Data analysis in R suing StarVZ framework.

* Methods 1/2
** Application: LU Factorization
- Decomposes matrix A into L (lower) and U (upper).
- Kernels used:
  - DGEMM – matrix multiplication
  - DTRSM – triangular solve
  - DGETRF_NOPIV – LU without pivoting
- Hybrid execution:
  - Static inter-node block distribution
  - Dynamic intra-node scheduling via StarPU

* Methods 2/2
** Experimental Design
Square matrix size of 14.4K (double precision).
1. *Phase 1*: Tile size tuning (128–1600)
2. *Phase 2*: Full factorial 4×4 experiment
   - Schedulers: lws, random, dmda, dmdas
   - Distributions: 1×6, 2×3, 3×2, 6×1
3. *Phase 3*: Detailed trace analysis with StarVZ
   - Fixed =lws= scheduler
   - Varying PxQ (BC) parameters.

* Results: Optimal Block Size
- Tested 10 tile sizes with =lws= scheduler.
- Best performance at *360×360* blocks.

#+ATTR_LATEX: :width 0.3\textwidth
[[../img/block-size.pdf]]

* Results: Scheduler and Data Partition Comparison
- Varying data distribution and schedulers.
- 1\times6 and 6\times1: \approx3900 MPI operations.
- 2\times3 and 3\times2: \approx2300 MPI operations.
- Similar makespans across different data distributions and schedulers.

#+ATTR_LATEX: :width 0.3\textwidth
[[../img/distrib-scheduler.pdf]]

* Results: OpenMPI Delays 1/2
- Fixed =lws= scheduler, varying data distributions.
- More dense behavior of =dgemm= tasks until 10s.
  - 1×6 (worst): mean idle \approx1500 ms
  - 2×3 (best): mean idle \approx800 ms
- Bottleneck from network latency, not algorithmic imbalance.

* Results: OpenMPI Delays 2/2
#+ATTR_LATEX: :width 0.3\textwidth
[[../img/lws-all_pq-traces.pdf]]

* Results: Schedulers Comparison
- Fixed 3×2 distribution, =lws= and =dmdas= schedulers.
- Per-node optimistic makespan (ABE):
  - \approx14605 ms for =lws=
  - \approx15045 ms for =dmdas=
- LWS temporal gaps between tasks \approx7841 ms; =dmdas= \approx9154 ms.
- 200 more outlier tasks in =dmdas= explain longer runtime.

#+ATTR_LATEX: :width 0.4\textwidth
[[../img/lws-vs-dmdas-3x2-openmpi-traces.pdf]]

* Results: Data Distribution Similarities
- Makespan difference <3%.
- ABE difference between nodes 1 and 2:
  - 3\times2: \approx2249 ms
  - 6\times1: \approx2194 ms
- Similar load imbalance between the two.

#+ATTR_LATEX: :width 0.45\textwidth
[[../img/lws-3x2-versus-6x1-openmpi-traces.pdf]]

* Conclusion and Future Work
- Impact of data distribution and scheduler heuristics.
- =dmda= and =dmdas= presented similar performances.
- =lws= best performing scheduler.
- Data partition (P\timesQ) had minimal impact on performance.
- Scale experiments using SimGrid simulation.
- Study repetitive network delays in StarPU-MPI.

* Acknowledgements
The experiments in this work used the PCAD infrastructure,
http://gppd-hpc.inf.ufrgs.br, at INF/UFRGS.  We also acknowledge the
Brazilian National Council for Scientific Technological Development
(CNPq) for their financial scholarship support. This study was
financed in part by the Coordenação de Aperfeiçoamento de Pessoal de
Nível Superior - Brasil (CAPES) - Finance Code 001, the FAPERGS
(16/354-8, 16/348-8), Petrobras (2020/00182-5).

#+BEGIN_EXPORT latex
\begin{figure}[h!]\centering
  \begin{minipage}{0.10\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/capes.png}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/cnpq.png}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/fapergs.jpg}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/petrobras.jpeg}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/ppgc.png}
  \end{minipage}
\end{figure}
#+END_EXPORT

* Contact

#+begin_center
Thank you for your attention!
#+end_center

#+begin_center
Otho José Sirtoli Marcondes <otho.marcondes@inf.ufrgs.br>

Lucas Mello Schnorr <schnorr@inf.ufrgs.br>

Phillipe Olivier Alexandre Navaux <navaux@inf.ufrgs.br>
#+end_center
