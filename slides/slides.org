# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+startup: beamer
#+STARTUP: overview
#+STARTUP: indent
#+TAGS: noexport(n)

#+TITLE: Impact of Data Distribution and Schedulers for @@latex: \linebreak@@ the LU Factorization on Multi-Core Clusters
#+SUBTITLE:  16th Workshop on Appl. for Multi-Core Architectures (WAMCA 2025)
#+AUTHOR: Otho José Sirtoli Marcondes, *Lucas M. Schnorr*, Philippe O. A. Navaux
#+EMAIL: no-email
#+DATE: October 28th, 2025 -- Bonito, MS -- Brazil

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [12pt,presentation, aspectratio=169]
#+BEAMER_THEME: metropolis [numbering=fraction, progressbar=frametitle, sectionpage=none]
#+OPTIONS:   H:1 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t title:nil

#+LATEX_HEADER: %\usepackage[utf8]{inputenc}
#+LATEX_HEADER: %\usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{palatino}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage[font=scriptsize,labelfont=bf]{caption}
#+LATEX_HEADER: \usepackage[absolute,overlay]{textpos}

#+LATEX_HEADER: \institute{Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Brazil \\
#+LATEX_HEADER:\begin{center}
#+LATEX_HEADER:\includegraphics[width=.12\textwidth]{img/UFRGS.pdf}%
#+LATEX_HEADER:\hspace{.5 cm}%
#+LATEX_HEADER:\includegraphics[width=.12\textwidth]{img/ppgc.png}%
#+LATEX_HEADER:\hspace{.5 cm}%
#+LATEX_HEADER:\raisebox{0.5\height}{\includegraphics[width=.12\textwidth]{img/cnpq.png}}%
#+LATEX_HEADER:\hspace{.5 cm}%
#+LATEX_HEADER:\raisebox{0.2\height}{\includegraphics[width=.12\textwidth]{img/capes.png}}%
#+LATEX_HEADER:\hspace{3cm}~
#+LATEX_HEADER:\end{center}
#+LATEX_HEADER:}

#+latex: \setbeamercolor{background canvas}{bg=white}
#+latex: \setbeamerfont{title}{size=\large}
#+latex: \setbeamerfont{subtitle}{size=\small}

#+LATEX_HEADER: \definecolor{mblue}{HTML}{005c8b} 
#+LATEX_HEADER: \definecolor{morange}{HTML}{f58431}

#+LATEX: \setbeamercolor{normal text}{% 
#+LATEX:   fg=mblue, 
#+LATEX:   bg=black!2 
#+LATEX: } 
 
#+LATEX: \setbeamercolor{alerted text}{%
#+LATEX:   fg=morange
#+LATEX: } 

#+LATEX: {
#+LATEX:  \maketitle
#+LATEX: }

#+LaTeX: %\setbeamertemplate{footline}[text line]{%
#+LaTeX: %  \parbox{\linewidth}{\vspace*{-8pt}\hspace{-1cm}\hfill ICPADS 2020 - Communication-Aware Load Balancing of the LU Factorization over Heterogeneous Clusters \hfill\insertframenumber~/ \inserttotalframenumber}}
#+LaTeX:  \setbeamertemplate{navigation symbols}{}

#+LaTeX: \newcommand\boldblue[1]{\textcolor{erad20blue}{\textbf{#1}}}
#+LaTeX: \newcommand\itred[1]{\textcolor{red}{\textit{#1}}}

* Figure                                                           :noexport:
** Block-cyclic
*** Compute the distributions

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
def_node_topology <- function(P = 2, Q = 3)
{
  tibble(p = 0:(P-1)) |>
    crossing(tibble(q = 0:(Q-1))) |>
    mutate(Node = 1:(P*Q))
}
def_matrix_topology <- function(M = 16, N = 16)
{
  tibble(X = 0:(M-1)) |>
    crossing(tibble(Y = 0:(N-1)))
}
def_distribution <- function(df.topo, df.matrix) {
  P = df.topo |> distinct(p) |> nrow()
  Q = df.topo |> distinct(q) |> nrow()
  df.matrix |>
    mutate(p = X %% P, q = Y %% Q) |>
    left_join(df.topo, by = join_by(p, q))
}
tribble(~P, ~Q,
        2, 3,
        3, 2,
        1, 6,
        6, 1) |>
  mutate(KEY = paste0(P, "x", Q)) |>
  mutate(TOPO = map2(P, Q, def_node_topology)) |>
  mutate(M = 16, N = 16) |>
  mutate(MATR = map2(M, N, def_matrix_topology)) |>
  mutate(OUT2 = map2(TOPO, MATR, def_distribution)) -> df
#+end_src

#+RESULTS:

*** Theme

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
default_theme <- function(base_size = 22, expand = 0.0, legend_title = FALSE, skip_x = FALSE) {
  ret <- list()

  ret[[length(ret) + 1]] <- theme_bw(base_size = base_size)
  ret[[length(ret) + 1]] <- theme(
    plot.margin = unit(c(0, 0, 0, 0), "cm"),
    legend.spacing = unit(3, "cm"),
    legend.position = "top",
    legend.justification = "left",
    legend.box.spacing = unit(0, "pt"),
    legend.box.margin = margin(0, 0, 0, 0)
  )
  ret[[length(ret) + 1]] <- guides(color = guide_legend(nrow = 1))
  if (!legend_title) {
    ret[[length(ret) + 1]] <- theme(legend.title = element_blank())
  }
  return(ret)
}
#+end_src

#+RESULTS:

*** Function

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
do_the_d <- function(df, key, white = FALSE)
{
df |>
  ggplot(aes(x=X,
             xmin=X,
             xmax=X+1,
             y=Y,
             ymin=Y,
             ymax=Y+1,
             fill=as.factor(Node))) +
  scale_fill_brewer(palette = "Set1") -> p.aux
if(white) {
  p.aux + geom_rect(color="black", fill="NA") -> p.aux
}else{
  p.aux + geom_rect(color="black") -> p.aux
}
p.aux +
  default_theme() + #base_size=18) +
  guides(fill = guide_legend(nrow = 1, override.aes = list(alpha=1))) -> p.aux
if(white) {
  p.aux + ggtitle("Matrix") -> p.aux
  p.aux + xlab("X Tile Coordinate") + ylab("Y Tile Coordinate") -> p.aux
}else{
  p.aux + ggtitle(key) -> p.aux
  p.aux + xlab("") + ylab("") -> p.aux
}
p.aux +
  theme(legend.position="none") +
  scale_x_continuous(breaks = seq(0,16, by=3)) +
  scale_y_reverse(breaks = seq(0,16, by=3))
}
#+end_src

#+RESULTS:

*** Several

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
library(patchwork)
df |>
  print() |>
  arrange(KEY) |>
  mutate(GG2 = map2(OUT2, KEY, do_the_d, white=FALSE)) |>
  mutate(FILENAME = paste0("img/bc-", P, ".pdf")) |>
  mutate(SAVE = map2(FILENAME, GG2, ggsave, width=5, height=5)) -> df.aux
#(df.aux$GG1[[1]] / df.aux$GG2) +
#  plot_layout(ncol=5, guides="collect") &
#  theme(legend.position="top") -> p
#+end_src

#+RESULTS:
: # A tibble: 4 × 8
:       P     Q KEY   TOPO                 M     N MATR               OUT2              
:   <dbl> <dbl> <chr> <list>           <dbl> <dbl> <list>             <list>            
: 1     2     3 2x3   <tibble [6 × 3]>    16    16 <tibble [256 × 2]> <tibble [256 × 5]>
: 2     3     2 3x2   <tibble [6 × 3]>    16    16 <tibble [256 × 2]> <tibble [256 × 5]>
: 3     1     6 1x6   <tibble [6 × 3]>    16    16 <tibble [256 × 2]> <tibble [256 × 5]>
: 4     6     1 6x1   <tibble [6 × 3]>    16    16 <tibble [256 × 2]> <tibble [256 × 5]>

*** White Matrix

#+begin_src R :results output :session *R* :exports both :noweb yes :colnames yes
options(crayon.enabled=FALSE)
library(tidyverse)
df |>
  slice(1) |>
  mutate(P = 0) |>
  mutate(GG = map2(OUT2, KEY, do_the_d, white=TRUE)) |>
  mutate(FILENAME = paste0("img/bc-", P, ".pdf")) |>
  mutate(SAVE = map2(FILENAME, GG, ggsave, width=5, height=5)) -> df.aux  
#+end_src

#+RESULTS:

* High-Performance Computing (HPC) Context

HPC is the backbone for groundbreaking research
- Computational physics, Climate modeling, Weather prediction @@latex:\pause@@
- Another example: Precision Agriculture
  - Need to interpolate several samples for decision-making
  - How much (and which type) of fertilizer to put in the soil

#+latex: \pause

Dense linear algebra (e.g., LU factorization) is a core workload
- Main compute-bound phase of many HPC iterative applications
- Platform: large HPC clusters composed of many nodes

* Load balancing across nodes is a critical challenge 1/2

Static data distribution, where the common method is the *Block-Cyclic*
- Balances computation cost using a P \times Q grid; with 6 nodes, we have:

#+latex: \bigskip

** Left                                                              :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.4
:END:
#+begin_export latex
\only<1>{\includegraphics[width=\linewidth]{./img/bc-0.pdf}}
\only<2>{\includegraphics[width=\linewidth]{./img/bc-6.pdf}}
\only<3>{\includegraphics[width=\linewidth]{./img/bc-2.pdf}}
\only<4>{\includegraphics[width=\linewidth]{./img/bc-3.pdf}}
\only<5->{\includegraphics[width=\linewidth]{./img/bc-1.pdf}}
#+end_export

#+begin_export latex
%\only<1->{\includegraphics[width=0.193\linewidth]{./img/bc-0.pdf}}
%\only<2->{\includegraphics[width=0.193\linewidth]{./img/bc-6.pdf}}
%\only<3->{\includegraphics[width=0.193\linewidth]{./img/bc-2.pdf}}
%\only<4->{\includegraphics[width=0.193\linewidth]{./img/bc-3.pdf}}
%\only<5->{\includegraphics[width=0.193\linewidth]{./img/bc-1.pdf}}
#+end_export

** Right
:PROPERTIES:
:BEAMER_col: 0.5
:END:

#+latex: \only<6->{

Excellent data locality

Lacks adaptability

Well-known limitations
- Prime node counts or
- Heterogeneous resources

#+latex: \vspace{.5cm}

ScaLAPACK

HPL benchmark (the Top500 list)

#+latex: }

* Load balancing across nodes is a critical challenge 2/2

- Dynamic scheduling: great adaptability but incurs overhead
  - Usually does not scale well for many nodes
  - Increased complexity for higher node counts (hierarchical sched.)

#+latex: \pause\vfill

- Hybrid strategies aim to combine both methods efficiently
  - Static data partitioning across nodes (the Block-Cyclic method)
  - Dynamic scheduling for intra-node (among CPUs and GPUs)

#+latex: \pause\vfill

** Environments employing the hybrid strategy                      :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

- The StarPU--MPI runtime
  - Adaptable
  - Performance portability
- The Chameleon framework
  - Dense linear algebra

* How the hybrid strategy (static/dynamic) method works

#+latex: \vspace{.1cm}

Developer express computations as a DAG

#+ATTR_LATEX: :width 0.4\textwidth
[[./img/dag-color-qr-dense-facto.pdf]]

#+latex: \pause\vfill

Developer decides static placement to nodes on task submission

#+latex: \pause\vspace{.2cm}

The runtime dynamically assign tasks to CPU/GPU workers on each node \\
Many scheduling heuristics possible (=lws=, =dmdas=, =eager=, ...) \\
#+latex: \pause  
Inter-node communication is transparent \to provides enormous flexibility

* Objective

#+begin_center
Analyze how

*data distribution* (P \times Q) and

*scheduling* *heuristics* (=lws=, =dmdas=, etc)

affect LU factorization performance

on a multi-code cluster
#+end_center

* Methodology 1/2
** Application: LU Factorization, decomposes A into L and U

#+latex: \smallskip
Doubled-precision blocked algorithm with tiles of order NB
#+attr_latex: :width .8\textwidth :center nil
[[../LU-factor.png]]  

=GEMM= (matrix mult.), =TRSM= (triang. solve), =GETRF_NOPIV= (LU no pivoting)

* Methodology 2/2
** Experimental Design for a square matrix (14.4K)

#+latex: \smallskip
*Phase 1*: Block size tuning (128 -- 1600) using one node

#+latex: \pause

*Phase 2*: Full factorial 4×4 experiment (the main part)
- Schedulers: lws, random, dmda, dmdas
- Distributions: 1×6, 2×3, 3×2, 6×1 (6 multi-code nodes)

#+latex: \pause

*Phase 3*: Detailed trace analysis with StarVZ
- Fixed =lws= scheduler
- Varying PxQ parameters

* Experimental Setup

** Hardware: Local Cluster Environment                             :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

#+attr_latex: :center nil
| Nodes           | 6                                           |
| Cores per node  | 24 (2\times Xeon Silver 4116 with 12 cores each) |
| Memory per node | 96 GB DDR4                                  |
| Network         | 10G Ethernet (X540-AT2)                     |

#+latex: \vspace{0.5cm}

** Software: GNU Guix for reproducible package management          :B_block:
:PROPERTIES:
:BEAMER_env: block
:END:

- CHAMELEON 1.3.0 for dense linear algebra
- StarPU-MPI 1.4.7 runtime system
- OpenMPI 4 transport layer
- Data analysis in R using the StarVZ framework

* Results: Optimal block size investigation

Eval. with 10 different block sizes with =lws= scheduler using a single node


#+ATTR_LATEX: :width 0.47\textwidth :center nil
[[../img/block-size.pdf]]

Obvervation: best performance with blocks of order 360

* Results: Scheduler and Data Partition Comparison

Eval. with four schedulers and four static data partitioning schemes

** Left                                                              :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.55
:END:

Initial expectation: schemes 2\times3 or 3\times2 better than the other two

#+latex: \bigskip

1\times6 and 6\times1: \approx3900 MPI operations \\
2\times3 and 3\times2: \approx2300 MPI operations

#+latex: \bigskip

Observation: similar makespans across different data distributions and schedulers

** Right                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.45
:END:

#+latex: \bigskip

#+ATTR_LATEX: :width \linewidth :center nil
[[../img/distrib-scheduler.pdf]]

* Space-time view using the StarVZ framework

Understanding the elements of this trace visualization

#+attr_latex: :center nil :width \linewidth
[[../img/only_dmdas-3x2-openmpi-traces.pdf]]

* Results: OpenMPI Delays
** Left                                                              :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.63
:END:

#+latex: \onslide<1->{
Fixed =lws= scheduler, varying data distributions

#+latex: }\vspace{1cm}\onslide<2->{

More dense behavior of =dgemm= tasks until 10s
- 1×6 (worst): mean idle \approx1500 ms
- 2×3 (best): mean idle \approx800 ms

#+latex: }\vspace{1cm}\onslide<3->{

Observation: bottleneck from network latency

#+latex: }
** Right                                                             :BMCOL:
:PROPERTIES:
:BEAMER_col: 0.38
:END:

#+begin_export latex
\vspace{-1.2cm}\onslide<1->{
\includegraphics[width=\linewidth]{../img/lws-all_pq-traces.pdf}
}
#+end_export

* Results: Schedulers Comparison                                   :noexport:
- Fixed 3×2 distribution, =lws= and =dmdas= schedulers
- Per-node optimistic makespan (ABE):
  - \approx14605 ms for =lws=
  - \approx15045 ms for =dmdas=
- LWS temporal gaps between tasks \approx7841 ms; =dmdas= \approx9154 ms
- 200 more outlier tasks in =dmdas= explain longer runtime

#+ATTR_LATEX: :width 0.4\textwidth
[[../img/lws-vs-dmdas-3x2-openmpi-traces.pdf]]

* Results: Data Distribution Similarities                          :noexport:
- Makespan difference <3%
- ABE difference between nodes 1 and 2:
  - 3\times2: \approx2249 ms
  - 6\times1: \approx2194 ms
- Similar load imbalance between the two

#+ATTR_LATEX: :width 0.45\textwidth
[[../img/lws-3x2-versus-6x1-openmpi-traces.pdf]]

* Conclusion and Future Work

Conclusion
- Impact of data distribution and scheduler heuristics
  - =lws= best performing scheduler
  - Data partition (P \times Q) had minimal impact on performance
- Reason for that might be related to OpenMPI delays
  - Current investigation using alternative transport layers
    
#+latex: \vfill\pause

Future Work
- Scale experiments using SimGrid simulation
- Study repetitive network delays in StarPU-MPI

* Acknowledgements
The experiments in this work used the PCAD infrastructure,
http://gppd-hpc.inf.ufrgs.br, at INF/UFRGS.  We also acknowledge the
Brazilian National Council for Scientific Technological Development
(CNPq) for their financial scholarship support. This study was
financed in part by the Coordenação de Aperfeiçoamento de Pessoal de
Nível Superior - Brasil (CAPES) - Finance Code 001, the FAPERGS
(16/354-8, 16/348-8), Petrobras (2020/00182-5).

#+BEGIN_EXPORT latex
\begin{figure}[h!]\centering
  \begin{minipage}{0.10\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/capes.png}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/cnpq.png}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/fapergs.jpg}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/petrobras.jpeg}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/ppgc.png}
  \end{minipage}
\end{figure}
#+END_EXPORT

* Contact

#+begin_center
Thank you for your attention!
#+end_center

#+begin_center
Otho José Sirtoli Marcondes <otho.marcondes@inf.ufrgs.br>

Lucas Mello Schnorr <schnorr@inf.ufrgs.br>

Phillipe Olivier Alexandre Navaux <navaux@inf.ufrgs.br>
#+end_center
