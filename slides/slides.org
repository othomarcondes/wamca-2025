# -*- mode: org -*-
# -*- coding: utf-8 -*-
#+startup: beamer
#+STARTUP: overview
#+STARTUP: indent
#+TAGS: noexport(n)

#+TITLE: Impact of Data Distribution and Schedulers for @@latex: \linebreak@@ the LU Factorization on Multi-Core Clusters
#+SUBTITLE:  16th Workshop on Appl. for Multi-Core Architectures (WAMCA 2025)
#+AUTHOR: Otho José Sirtoli Marcondes, *Lucas M. Schnorr*, Philippe O. A. Navaux
#+EMAIL: no-email
#+DATE: October 28th, 2025 -- Bonito, MS -- Brazil

#+LaTeX_CLASS: beamer
#+LaTeX_CLASS_OPTIONS: [12pt,presentation, aspectratio=169]
#+BEAMER_THEME: metropolis [numbering=fraction, progressbar=frametitle, sectionpage=none]
#+OPTIONS:   H:1 num:t toc:nil \n:nil @:t ::t |:t ^:t -:t f:t *:t <:t title:nil

#+LATEX_HEADER: %\usepackage[utf8]{inputenc}
#+LATEX_HEADER: %\usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage{palatino}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage[font=scriptsize,labelfont=bf]{caption}
#+LATEX_HEADER: \usepackage[absolute,overlay]{textpos}

#+LATEX_HEADER: \institute{Institute of Informatics, Federal University of Rio Grande do Sul (UFRGS), Brazil \\
#+LATEX_HEADER:\begin{center}
#+LATEX_HEADER:\includegraphics[width=.12\textwidth]{img/UFRGS.pdf}%
#+LATEX_HEADER:\hspace{.5 cm}%
#+LATEX_HEADER:\includegraphics[width=.12\textwidth]{img/ppgc.png}%
#+LATEX_HEADER:\hspace{.5 cm}%
#+LATEX_HEADER:\raisebox{0.5\height}{\includegraphics[width=.12\textwidth]{img/cnpq.png}}%
#+LATEX_HEADER:\hspace{.5 cm}%
#+LATEX_HEADER:\raisebox{0.2\height}{\includegraphics[width=.12\textwidth]{img/capes.png}}%
#+LATEX_HEADER:\hspace{3cm}~
#+LATEX_HEADER:\end{center}
#+LATEX_HEADER:}

#+latex: \setbeamercolor{background canvas}{bg=white}
#+latex: \setbeamerfont{title}{size=\large}
#+latex: \setbeamerfont{subtitle}{size=\small}

#+LATEX_HEADER: \definecolor{mblue}{HTML}{005c8b} 
#+LATEX_HEADER: \definecolor{morange}{HTML}{f58431}

#+LATEX: \setbeamercolor{normal text}{% 
#+LATEX:   fg=mblue, 
#+LATEX:   bg=black!2 
#+LATEX: } 
 
#+LATEX: \setbeamercolor{alerted text}{%
#+LATEX:   fg=morange
#+LATEX: } 

#+LATEX: {
#+LATEX:  \maketitle
#+LATEX: }

#+LaTeX: %\setbeamertemplate{footline}[text line]{%
#+LaTeX: %  \parbox{\linewidth}{\vspace*{-8pt}\hspace{-1cm}\hfill ICPADS 2020 - Communication-Aware Load Balancing of the LU Factorization over Heterogeneous Clusters \hfill\insertframenumber~/ \inserttotalframenumber}}
#+LaTeX:  \setbeamertemplate{navigation symbols}{}

#+LaTeX: \newcommand\boldblue[1]{\textcolor{erad20blue}{\textbf{#1}}}
#+LaTeX: \newcommand\itred[1]{\textcolor{red}{\textit{#1}}}

* High-Performance Computing (HPC) Context

HPC is the backbone for groundbreaking research
- Computational physics, Climate modeling, Weather prediction @@latex:\pause@@
- Another example: Precision Agriculture
  - Need to interpolate several samples for decision-making
  - How much (and which type) of fertilizer to put in the soil

#+latex: \pause

Dense linear algebra (e.g., LU factorization) is a core workload
- Main compute-bound phase of many HPC iterative applications
- Platform: large HPC clusters composed of many nodes

# Modern clusters have thousands of multi-core nodes

* Introduction and Motivation
Load balancing across nodes is a critical challenge
- Static data distribution: excellent data locality but lacks adaptability
- Dynamic scheduling: great adaptability but incurs overhead
- Hybrid strategies aim to combine both efficiently (StarPU-MPI and CHAMELEON)
Objective
- Analyze how *data distribution* and *scheduling heuristics* affect LU factorization performance

* Related Work: Static Data Distribution
- *Block-Cyclic (BC)* used in ScaLAPACK, HPL benchmark
- Balances computation and communication via P×Q grid
- Limitations with prime node counts or heterogeneous resources
#+attr_latex: :center no :height 2.5cm
[[../img/bc.pdf]]

* Related Work: Task-Based Runtime Systems
- Frameworks: StarPU, PaRSEC, OmpSs
- Express computations as task DAGs
- Adaptability and performance portability
- Schedulers dynamically assign tasks

* Experimental Setup: Hardware Environment
| Resource | Specification |
|-----------+---------------|
| Cluster | PCAD @ INF/UFRGS |
| Nodes | 6 |
| Cores per node | 24 (2×12 Xeon Silver 4116) |
| Memory per node | 96 GB DDR4 |
| Network | 10G Ethernet (X540-AT2) |

* Experimental Setup: Software Environment
- CHAMELEON 1.3.0 for dense linear algebra
- StarPU-MPI 1.4.7 runtime system
- OpenMPI 4 transport layer
- GNU Guix for reproducible package management
- Data analysis in R suing StarVZ framework

* Methods 1/2
** Application: LU Factorization
- Decomposes matrix A into L (lower) and U (upper)
- Kernels used:
  - DGEMM – matrix multiplication
  - DTRSM – triangular solve
  - DGETRF_NOPIV – LU without pivoting
- Hybrid execution:
  - Static inter-node block distribution
  - Dynamic intra-node scheduling via StarPU

* Methods 2/2
** Experimental Design
Square matrix size of 14.4K (double precision)
1. *Phase 1*: Tile size tuning (128–1600)
2. *Phase 2*: Full factorial 4×4 experiment
   - Schedulers: lws, random, dmda, dmdas
   - Distributions: 1×6, 2×3, 3×2, 6×1
3. *Phase 3*: Detailed trace analysis with StarVZ
   - Fixed =lws= scheduler
   - Varying PxQ (BC) parameters

* Results: Optimal Block Size
- Tested 10 tile sizes with =lws= scheduler
- Best performance at *360×360* blocks

#+ATTR_LATEX: :width 0.3\textwidth
[[../img/block-size.pdf]]

* Results: Scheduler and Data Partition Comparison
- Varying data distribution and schedulers
- 1\times6 and 6\times1: \approx3900 MPI operations
- 2\times3 and 3\times2: \approx2300 MPI operations
- Similar makespans across different data distributions and schedulers

#+ATTR_LATEX: :width 0.3\textwidth
[[../img/distrib-scheduler.pdf]]

* Results: OpenMPI Delays 1/2
- Fixed =lws= scheduler, varying data distributions
- More dense behavior of =dgemm= tasks until 10s
  - 1×6 (worst): mean idle \approx1500 ms
  - 2×3 (best): mean idle \approx800 ms
- Bottleneck from network latency, not algorithmic imbalance

* Results: OpenMPI Delays 2/2
#+ATTR_LATEX: :width 0.3\textwidth
[[../img/lws-all_pq-traces.pdf]]

* Results: Schedulers Comparison
- Fixed 3×2 distribution, =lws= and =dmdas= schedulers
- Per-node optimistic makespan (ABE):
  - \approx14605 ms for =lws=
  - \approx15045 ms for =dmdas=
- LWS temporal gaps between tasks \approx7841 ms; =dmdas= \approx9154 ms
- 200 more outlier tasks in =dmdas= explain longer runtime

#+ATTR_LATEX: :width 0.4\textwidth
[[../img/lws-vs-dmdas-3x2-openmpi-traces.pdf]]

* Results: Data Distribution Similarities
- Makespan difference <3%
- ABE difference between nodes 1 and 2:
  - 3\times2: \approx2249 ms
  - 6\times1: \approx2194 ms
- Similar load imbalance between the two

#+ATTR_LATEX: :width 0.45\textwidth
[[../img/lws-3x2-versus-6x1-openmpi-traces.pdf]]

* Conclusion and Future Work
- Impact of data distribution and scheduler heuristics
- =dmda= and =dmdas= presented similar performances
- =lws= best performing scheduler
- Data partition (P \times Q) had minimal impact on performance
- Scale experiments using SimGrid simulation
- Study repetitive network delays in StarPU-MPI

* Acknowledgements
The experiments in this work used the PCAD infrastructure,
http://gppd-hpc.inf.ufrgs.br, at INF/UFRGS.  We also acknowledge the
Brazilian National Council for Scientific Technological Development
(CNPq) for their financial scholarship support. This study was
financed in part by the Coordenação de Aperfeiçoamento de Pessoal de
Nível Superior - Brasil (CAPES) - Finance Code 001, the FAPERGS
(16/354-8, 16/348-8), Petrobras (2020/00182-5).

#+BEGIN_EXPORT latex
\begin{figure}[h!]\centering
  \begin{minipage}{0.10\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/capes.png}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/cnpq.png}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/fapergs.jpg}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/petrobras.jpeg}
  \end{minipage}
  \begin{minipage}{0.19\textwidth}\centering
    \includegraphics[width=\linewidth]{./logo/ppgc.png}
  \end{minipage}
\end{figure}
#+END_EXPORT

* Contact

#+begin_center
Thank you for your attention!
#+end_center

#+begin_center
Otho José Sirtoli Marcondes <otho.marcondes@inf.ufrgs.br>

Lucas Mello Schnorr <schnorr@inf.ufrgs.br>

Phillipe Olivier Alexandre Navaux <navaux@inf.ufrgs.br>
#+end_center
